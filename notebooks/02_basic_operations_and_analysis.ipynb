{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version compatibility check\n",
    "\n",
    "This notebook compares the xsnow package installed in your environment with the documentation version it was written for.",
    " The helper below calls `scripts/check_docs_version.py` so you can confirm that the package and docs align before continuing.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "\n",
    "def _find_script() -> Path | None:\n",
    "    current = Path.cwd().resolve()\n",
    "    for candidate in [current, *current.parents]:\n",
    "        script = candidate / \"scripts\" / \"check_docs_version.py\"\n",
    "        if script.exists():\n",
    "            return script\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_docs_version() -> tuple[str | None, str | None]:\n",
    "    script_path = _find_script()\n",
    "    if script_path is None:\n",
    "        return None, \"scripts/check_docs_version.py was not found\"\n",
    "    try:\n",
    "        completed = subprocess.run(\n",
    "            [sys.executable, str(script_path)],\n",
    "            check=True,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "        )\n",
    "    except subprocess.CalledProcessError as exc:\n",
    "        output = (exc.stdout or \"\") + (exc.stderr or \"\")\n",
    "        return None, output.strip() or str(exc)\n",
    "    return completed.stdout.strip() or None, None\n",
    "\n",
    "\n",
    "docs_version, docs_error = get_docs_version()\n",
    "\n",
    "try:\n",
    "    import xsnow\n",
    "    package_version = xsnow.__version__\n",
    "except Exception as exc:  # pylint: disable=broad-except\n",
    "    xsnow = None  # type: ignore[assignment]\n",
    "    package_version = None\n",
    "    package_error = str(exc)\n",
    "else:\n",
    "    package_error = None\n",
    "\n",
    "print(f\"xsnow package version: {package_version if package_version else 'not installed'}\")\n",
    "if package_error and not package_version:\n",
    "    print(f\"Import error: {package_error}\")\n",
    "\n",
    "if docs_version:\n",
    "    print(f\"xsnow docs version: {docs_version}\")\n",
    "else:\n",
    "    message = \"xsnow docs version: unavailable\"\n",
    "    if docs_error:\n",
    "        message += f\" ({docs_error})\"\n",
    "    print(message)\n",
    "\n",
    "if docs_version and package_version and docs_version != package_version:\n",
    "    warnings.warn(\n",
    "        \"xsnow package version differs from the documentation version. \"\n",
    "        \"Consider aligning them before executing the notebook.\",\n",
    "        stacklevel=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02: Basic Operations and Analysis\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Austfi/xsnowForPatrol/blob/main/notebooks/02_basic_operations_and_analysis.ipynb)\n",
    "\n",
    "Now that you understand xsnow's data structure, let's learn how to work with the data: selecting, filtering, and performing basic analyses.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Selecting data by location, time, and layer\n",
    "- Filtering data with conditions\n",
    "- Computing profile-level summaries\n",
    "- Calculating snow water equivalent (SWE)\n",
    "- Identifying weak layers\n",
    "- Time series operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning objectives\n",
    "- Select xsnow data by location, time, and depth to isolate relevant layers.\n",
    "- Filter layers with conditional masks to spotlight weak or dense snow.\n",
    "- Compute profile summaries and SWE estimates for decision support.\n",
    "- Translate xsnow objects to familiar NumPy arrays for downstream tools.\n",
    "\n",
    "**Prerequisites**\n",
    "- [ ] Familiarity with xsnow basics from Notebook 01.\n",
    "- [ ] Comfort indexing pandas/xarray objects.\n",
    "- [ ] Ability to interpret scientific units like density and SWE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation (For Colab Users)\n",
    "\n",
    "Set `INSTALL_XSNOW = True` in the next cell if you need to install xsnow. ",
    "When enabled you can pick `INSTALL_METHOD = \"pip\"` to install published packages or `INSTALL_METHOD = \"dev\"` to work from a local clone.",
    " The cell also installs the supporting scientific Python stack used throughout the course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "INSTALL_XSNOW = False  # Set to True to install or update xsnow in this environment.\n",
    "INSTALL_METHOD = \"pip\"  # Choose \"pip\" for a package install, or \"dev\" for a developer clone.\n",
    "DEV_REPO_URL = \"https://gitlab.com/avacollabra/postprocessing/xsnow.git\"\n",
    "DEV_CLONE_DIR = Path.home() / \"xsnow-dev\"\n",
    "\n",
    "\n",
    "def _run(cmd: list[str]) -> None:\n",
    "    print(f\"$ {' '.join(cmd)}\")\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "\n",
    "try:\n",
    "    import xsnow\n",
    "    print(f\"xsnow {xsnow.__version__} is already available.\")\n",
    "except Exception as exc:  # pylint: disable=broad-except\n",
    "    xsnow = None  # type: ignore[assignment]\n",
    "    print(f\"xsnow is not currently available: {exc}\")\n",
    "    if not INSTALL_XSNOW:\n",
    "        print(\"Set INSTALL_XSNOW = True and re-run this cell to install xsnow (pip or dev clone).\")\n",
    "    else:\n",
    "        try:\n",
    "            if INSTALL_METHOD == \"pip\":\n",
    "                _run([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"numpy\", \"pandas\", \"xarray\", \"matplotlib\", \"seaborn\", \"dask\", \"netcdf4\"])\n",
    "                _run([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"git+https://gitlab.com/avacollabra/postprocessing/xsnow\"])\n",
    "            elif INSTALL_METHOD == \"dev\":\n",
    "                if not DEV_CLONE_DIR.exists():\n",
    "                    _run([\"git\", \"clone\", DEV_REPO_URL, str(DEV_CLONE_DIR)])\n",
    "                _run([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"-e\", str(DEV_CLONE_DIR)])\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported INSTALL_METHOD: {INSTALL_METHOD}\")\n",
    "        except subprocess.CalledProcessError as install_error:\n",
    "            raise RuntimeError(\"xsnow installation command failed\") from install_error\n",
    "        import xsnow  # noqa: F401  # pylint: disable=import-outside-toplevel\n",
    "        print(f\"xsnow {xsnow.__version__} installed successfully.\")\n",
    "else:\n",
    "    INSTALL_XSNOW = INSTALL_XSNOW  # no-op so variable is defined for later cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Load Sample Data\n",
    "**Show.** We'll pull the bundled dataset so every selection example has real coordinates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run.\n",
    "import xsnow\n",
    "import numpy as np\n",
    "\n",
    "# Load sample data\n",
    "print(\"Loading sample data...\")\n",
    "try:\n",
    "    ds = xsnow.single_profile_timeseries()\n",
    "    print(\"\u2705 Data loaded!\")\n",
    "    print(f\"Dimensions: {dict(ds.dims)}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error: {e}\")\n",
    "    print(\"Install: pip install git+https://gitlab.com/avacollabra/postprocessing/xsnow\")\n",
    "    ds = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** A quick helper call gives us a multi-dimensional dataset with location, slope, realization, time, and layer axes.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: dataset ready\n",
    "assert ds is not None, 'Install xsnow before proceeding.'\n",
    "assert 'location' in ds.dims\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Selecting Data\n",
    "**Show.** We'll grab slices by location, time, and depth to practice navigation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run.\n",
    "# Select a specific location\n",
    "if len(ds.coords['location']) > 0:\n",
    "    location_name = ds.coords['location'].values[0]\n",
    "    ds_site = ds.sel(location=location_name)\n",
    "\n",
    "# Select a specific time (or time range)\n",
    "if len(ds.coords['time']) > 0:\n",
    "    # Get first and last time\n",
    "    times = ds.coords['time'].values\n",
    "    \n",
    "    # Select a single time\n",
    "    ds_single_time = ds.sel(time=times[0])\n",
    "    \n",
    "    # Select a time range using slice\n",
    "    ds_time_range = ds.sel(time=slice(times[0], times[9]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** Named selections with `.sel` keep coordinate labels intact so you always know which site and timestamp you're analyzing.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: selection shapes\n",
    "assert 'location' not in ds_site.dims\n",
    "assert ds_single_time.sizes['layer'] == ds.sizes['layer']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting by Index with `.isel()`\n",
    "**Show.** Use `.isel()` to pull positional slices along time and layer axes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run.\n",
    "# Get first time step\n",
    "ds_first = ds.isel(time=0)\n",
    "\n",
    "# Get first 5 time steps\n",
    "ds_first5 = ds.isel(time=slice(0, 5))\n",
    "\n",
    "# Get surface layer (layer 0)\n",
    "surface = ds.isel(layer=0)\n",
    "\n",
    "# Get multiple layers\n",
    "top_layers = ds.isel(layer=[0, 1, 2])  # Top 3 layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** `.isel` provides quick positional slicing when you just need the first few entries without worrying about coordinate labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: positional slicing\n",
    "assert ds_first.sizes['time'] == 1\n",
    "assert top_layers.sizes['layer'] == 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting by Depth (using 'z' coordinate)\n",
    "**Show.** Filter layers by physical depth using the signed `z` coordinate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run.\n",
    "# Select layers within 50 cm of surface (z between 0 and -0.5)\n",
    "# Note: z is negative downward, so we want z >= -0.5\n",
    "shallow = ds.where((ds.coords['z'] >= -0.5) & (ds.coords['z'] <= 0), drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** Combining boolean masks with `.where(..., drop=True)` filters the dataset to just the physical layers you care about.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: depth filter\n",
    "assert bool((shallow.coords['z'] >= -0.5).all())\n",
    "assert shallow.dims.get('layer', 0) <= ds.dims['layer']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Filtering Data with Conditions\n",
    "**Show.** Build boolean masks to spotlight layers that meet safety criteria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run.\n",
    "# Find layers with density > 300 kg/m\u00b3 (dense snow)\n",
    "dense_mask = ds['density'] > 300\n",
    "dense_layers = ds.where(dense_mask, drop=True)\n",
    "\n",
    "\n",
    "# Find cold layers (temperature < -10\u00b0C)\n",
    "if 'temperature' in ds.data_vars:\n",
    "    cold_mask = ds['temperature'] < -10\n",
    "    cold_layers = ds.where(cold_mask, drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** Density-based masks reveal crusts or heavy slabs while preserving contextual coordinates.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: dense layer mask\n",
    "assert float(dense_layers['density'].min()) >= 300\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Computing Profile-Level Summaries\n",
    "**Show.** Aggregate layer data into profile summaries for quick dashboards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run.\n",
    "# Compute mean density across all layers for each profile\n",
    "mean_density = ds['density'].mean(dim='layer')\n",
    "\n",
    "# Other useful aggregations\n",
    "max_density = ds['density'].max(dim='layer')  # Maximum density in profile\n",
    "min_density = ds['density'].min(dim='layer')   # Minimum density in profile\n",
    "std_density = ds['density'].std(dim='layer')   # Standard deviation\n",
    "\n",
    "\n",
    "# Add as new variable to dataset\n",
    "ds = ds.assign(mean_density=mean_density)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** Aggregations collapse the layer axis so you can compare profiles over time or across locations at a glance.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: mean density shape\n",
    "assert 'layer' not in mean_density.dims\n",
    "assert 'mean_density' in ds.data_vars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Calculating Snow Water Equivalent (SWE)\n",
    "**Show.** Explore strategies to approximate SWE depending on available variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run.\n",
    "# Method 1: If we have layer thickness directly\n",
    "if 'thickness' in ds.data_vars:\n",
    "    # SWE = sum(density * thickness) over all layers\n",
    "    swe = (ds['density'] * ds['thickness']).sum(dim='layer') / 1000.0  # Convert to m\n",
    "    print(f\"SWE calculated from thickness: {swe.values[0]:.3f} m (first profile)\")\n",
    "    ds = ds.assign(SWE=swe)\n",
    "\n",
    "# Method 2: Compute thickness from 'z' coordinate (depth)\n",
    "elif 'z' in ds.coords and 'density' in ds.data_vars:\n",
    "    # Layer thickness = difference in z between adjacent layers\n",
    "    # For each layer, thickness = |z[i] - z[i+1]|, except last layer\n",
    "    z = ds.coords['z']\n",
    "    \n",
    "    # Compute thickness by differencing z (absolute value since z is negative)\n",
    "    z_diff = z.diff(dim='layer', label='upper')\n",
    "    thickness = -z_diff  # Negative because z decreases downward\n",
    "    \n",
    "    # For the last layer, estimate thickness from remaining depth\n",
    "    # This is a simplified approach - in practice, you'd need more sophisticated handling\n",
    "    if 'HS' in ds.data_vars:\n",
    "        # Use HS to estimate remaining thickness for last layer\n",
    "        total_thickness = -z.isel(layer=0)  # Depth of first layer\n",
    "        computed_thickness = thickness.sum(dim='layer', skipna=True)\n",
    "        last_layer_thickness = total_thickness - computed_thickness\n",
    "        \n",
    "        # Fill NaN values in thickness with estimated last layer thickness\n",
    "        thickness = thickness.fillna(last_layer_thickness)\n",
    "        \n",
    "        # Calculate SWE\n",
    "        swe = (ds['density'] * thickness).sum(dim='layer') / 1000.0  # Convert to m\n",
    "        print(f\"SWE calculated from z coordinate: {swe.values[0]:.3f} m (first profile)\")\n",
    "        ds = ds.assign(SWE=swe)\n",
    "    else:\n",
    "        print(\"Note: HS not available, using simplified thickness calculation\")\n",
    "        swe = (ds['density'] * thickness.fillna(0)).sum(dim='layer') / 1000.0\n",
    "        ds = ds.assign(SWE=swe)\n",
    "\n",
    "# Method 3: If SWE is already in the dataset\n",
    "if 'SWE' in ds.data_vars or 'swe' in ds.data_vars:\n",
    "    swe_var = ds['SWE'] if 'SWE' in ds.data_vars else ds['swe']\n",
    "    print(f\"SWE found in dataset: {swe_var.values[0]:.3f} m (first profile)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** SWE estimates depend on which supporting variables exist, so the notebook demonstrates fallback strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: SWE placeholders\n",
    "if 'SWE' in ds.data_vars:\n",
    "    assert swe_var.dims == ds['SWE'].dims\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Identifying Weak Layers\n",
    "**Show.** Combine density and grain clues to flag weak layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run.\n",
    "# Define weak layer criteria\n",
    "# This is a simplified example - real weak layer identification is more complex\n",
    "\n",
    "weak_mask = None\n",
    "\n",
    "if 'density' in ds.data_vars:\n",
    "    # Weak layers often have low density (< 150 kg/m\u00b3)\n",
    "    low_density = ds['density'] < 150\n",
    "    \n",
    "    if 'grain_type' in ds.data_vars:\n",
    "        # Some grain types indicate weak layers (this depends on your grain type coding)\n",
    "        # Example: grain_type == 4 might indicate faceted crystals\n",
    "        # Note: Check your data's grain type coding scheme!\n",
    "        # weak_grain = ds['grain_type'] == 4  # Example only\n",
    "        weak_mask = low_density  # Simplified: just use density for now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** Even simple heuristics (like low density) can highlight suspect layers for deeper investigation.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: weak mask created\n",
    "assert weak_mask is not None\n",
    "assert weak_mask.dims == ds['density'].dims\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Time Series Operations\n",
    "**Show.** Analyze change over time by averaging, slicing, and differencing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run.\n",
    "# Compute mean over time (average profile)\n",
    "if 'density' in ds.data_vars:\n",
    "    mean_density_over_time = ds['density'].mean(dim='time')\n",
    "\n",
    "# Compute time series of profile-level variables\n",
    "if 'HS' in ds.data_vars:\n",
    "    hs_series = ds['HS'].isel(location=0, slope=0, realization=0)\n",
    "\n",
    "# Compute change over time (difference between consecutive time steps)\n",
    "if 'HS' in ds.data_vars:\n",
    "    hs_change = ds['HS'].diff(dim='time')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** Time-based reductions and differences expose evolving snowpack structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: time operations\n",
    "if 'density' in ds.data_vars:\n",
    "    assert 'layer' in mean_density_over_time.dims\n",
    "if 'HS' in ds.data_vars:\n",
    "    assert hs_change.dims['time'] == ds.dims['time'] - 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Values as NumPy Arrays\n",
    "**Show.** Convert xarray-backed data into bare NumPy arrays for external tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run.\n",
    "# Get values as NumPy array\n",
    "density_array = ds['density'].values\n",
    "\n",
    "# Get values for a specific selection\n",
    "surface_density = ds['density'].sel(layer=0).values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** Pulling NumPy arrays lets you hand data to scikit-learn, SciPy, or custom simulations.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: numpy access\n",
    "assert density_array.ndim >= 1\n",
    "assert surface_density.shape[0] == ds.dims['time']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play\n",
    "Adjust the density threshold or depth window to explore how selections change. Keep runs lightweight by sampling a single location.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run.\n",
    "density_cutoff = 280  # Try between 250 and 320\n",
    "depth_limit = -0.6  # Try between -0.2 and -1.0\n",
    "\n",
    "subset = ds.isel(location=0).where(ds['density'] > density_cutoff, drop=True)\n",
    "shallow_subset = subset.where(ds.coords['z'] >= depth_limit, drop=True)\n",
    "print('Layers retained:', shallow_subset.dims.get('layer', 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "Test yourself with these prompts before opening the solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a mask for layers where temperature is warmer than -5\u00b0C and inspect the remaining density.\n",
    "2. Compute the rolling 3-step mean of `HS` for a single location.\n",
    "3. Export a selection of `density` to a pandas DataFrame and describe the index levels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solutions</summary>\n",
    "\n",
    "1. `warm = ds.where(ds['temperature'] > -5, drop=True)` then inspect `warm['density']`.\n",
    "2. `ds['HS'].isel(location=0, slope=0, realization=0).rolling(time=3).mean()` demonstrates smoothing.\n",
    "3. `ds['density'].isel(location=0).to_dataframe().head()` reveals the MultiIndex structure.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Coordinate-based selections and boolean masks isolate layers for closer study.\n",
    "- Aggregations and SWE estimates condense stratigraphy into actionable metrics.\n",
    "- Exporting to NumPy unlocks interoperability with broader scientific tools.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}