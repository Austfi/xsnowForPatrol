{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02: Basic Operations and Analysis\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Austfi/xsnowForPatrol/blob/main/notebooks/02_basic_operations_and_analysis.ipynb)\n",
    "\n",
    "Now that you understand xsnow's data structure, let's learn how to work with the data: selecting, filtering, and performing basic analyses.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Selecting data by location, time, and layer\n",
    "- Filtering data with conditions\n",
    "- Computing profile-level summaries\n",
    "- Calculating snow water equivalent (SWE)\n",
    "- Identifying weak layers\n",
    "- Time series operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning objectives\n",
    "- Select xsnow data by location, time, and depth to isolate relevant layers.\n",
    "- Filter layers with conditional masks to spotlight weak or dense snow.\n",
    "- Compute profile summaries and SWE estimates for decision support.\n",
    "- Translate xsnow objects to familiar NumPy arrays for downstream tools.\n",
    "\n",
    "**Prerequisites**\n",
    "- [ ] Familiarity with xsnow basics from Notebook 01.\n",
    "- [ ] Comfort indexing pandas/xarray objects.\n",
    "- [ ] Ability to interpret scientific units like density and SWE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation (For Colab Users)\n",
    "\n",
    "If you're using Google Colab, run the cell below to install xsnow and dependencies. If you're running locally and have already installed xsnow, you can skip this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install -q numpy pandas xarray matplotlib seaborn dask netcdf4\n",
    "%pip install -q git+https://gitlab.com/avacollabra/postprocessing/xsnow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Load Sample Data\n",
    "**Show.** We'll pull the bundled dataset so every selection example has real coordinates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run.\n",
    "import xsnow\n",
    "import numpy as np\n",
    "\n",
    "# Load sample data\n",
    "print(\"Loading sample data...\")\n",
    "try:\n",
    "    ds = xsnow.single_profile_timeseries()\n",
    "    print(\"\u2705 Data loaded!\")\n",
    "    print(f\"Dimensions: {dict(ds.dims)}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error: {e}\")\n",
    "    print(\"Install: pip install git+https://gitlab.com/avacollabra/postprocessing/xsnow\")\n",
    "    ds = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** A quick helper call gives us a multi-dimensional dataset with location, slope, realization, time, and layer axes.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: dataset ready\n",
    "assert ds is not None, 'Install xsnow before proceeding.'\n",
    "assert 'location' in ds.dims\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Selecting Data\n",
    "**Show.** We'll grab slices by location, time, and depth to practice navigation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run.\n",
    "# Select a specific location\n",
    "if len(ds.coords['location']) > 0:\n",
    "    location_name = ds.coords['location'].values[0]\n",
    "    ds_site = ds.sel(location=location_name)\n",
    "\n",
    "# Select a specific time (or time range)\n",
    "if len(ds.coords['time']) > 0:\n",
    "    # Get first and last time\n",
    "    times = ds.coords['time'].values\n",
    "    \n",
    "    # Select a single time\n",
    "    ds_single_time = ds.sel(time=times[0])\n",
    "    \n",
    "    # Select a time range using slice\n",
    "    ds_time_range = ds.sel(time=slice(times[0], times[9]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** Named selections with `.sel` keep coordinate labels intact so you always know which site and timestamp you're analyzing.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: selection shapes\n",
    "assert 'location' not in ds_site.dims\n",
    "assert ds_single_time.sizes['layer'] == ds.sizes['layer']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting by Index with `.isel()`\n",
    "**Show.** Use `.isel()` to pull positional slices along time and layer axes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run.\n",
    "# Get first time step\n",
    "ds_first = ds.isel(time=0)\n",
    "\n",
    "# Get first 5 time steps\n",
    "ds_first5 = ds.isel(time=slice(0, 5))\n",
    "\n",
    "# Get surface layer (layer 0)\n",
    "surface = ds.isel(layer=0)\n",
    "\n",
    "# Get multiple layers\n",
    "top_layers = ds.isel(layer=[0, 1, 2])  # Top 3 layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** `.isel` provides quick positional slicing when you just need the first few entries without worrying about coordinate labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: positional slicing\n",
    "assert ds_first.sizes['time'] == 1\n",
    "assert top_layers.sizes['layer'] == 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting by Depth (using 'z' coordinate)\n",
    "**Show.** Filter layers by physical depth using the signed `z` coordinate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run.\n",
    "# Select layers within 50 cm of surface (z between 0 and -0.5)\n",
    "# Note: z is negative downward, so we want z >= -0.5\n",
    "shallow = ds.where((ds.coords['z'] >= -0.5) & (ds.coords['z'] <= 0), drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** Combining boolean masks with `.where(..., drop=True)` filters the dataset to just the physical layers you care about.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: depth filter\n",
    "assert bool((shallow.coords['z'] >= -0.5).all())\n",
    "assert shallow.dims.get('layer', 0) <= ds.dims['layer']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Filtering Data with Conditions\n",
    "**Show.** Build boolean masks to spotlight layers that meet safety criteria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run.\n",
    "# Find layers with density > 300 kg/m\u00b3 (dense snow)\n",
    "dense_mask = ds['density'] > 300\n",
    "dense_layers = ds.where(dense_mask, drop=True)\n",
    "\n",
    "\n",
    "# Find cold layers (temperature < -10\u00b0C)\n",
    "if 'temperature' in ds.data_vars:\n",
    "    cold_mask = ds['temperature'] < -10\n",
    "    cold_layers = ds.where(cold_mask, drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** Density-based masks reveal crusts or heavy slabs while preserving contextual coordinates.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: dense layer mask\n",
    "assert float(dense_layers['density'].min()) >= 300\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Computing Profile-Level Summaries\n",
    "**Show.** Aggregate layer data into profile summaries for quick dashboards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run.\n",
    "# Compute mean density across all layers for each profile\n",
    "mean_density = ds['density'].mean(dim='layer')\n",
    "\n",
    "# Other useful aggregations\n",
    "max_density = ds['density'].max(dim='layer')  # Maximum density in profile\n",
    "min_density = ds['density'].min(dim='layer')   # Minimum density in profile\n",
    "std_density = ds['density'].std(dim='layer')   # Standard deviation\n",
    "\n",
    "\n",
    "# Add as new variable to dataset\n",
    "ds = ds.assign(mean_density=mean_density)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** Aggregations collapse the layer axis so you can compare profiles over time or across locations at a glance.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: mean density shape\n",
    "assert 'layer' not in mean_density.dims\n",
    "assert 'mean_density' in ds.data_vars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Calculating Snow Water Equivalent (SWE)\n",
    "**Show.** Explore strategies to approximate SWE depending on available variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run.\n",
    "# Method 1: If we have layer thickness directly\n",
    "if 'thickness' in ds.data_vars:\n",
    "    # SWE = sum(density * thickness) over all layers\n",
    "    swe = (ds['density'] * ds['thickness']).sum(dim='layer') / 1000.0  # Convert to m\n",
    "\n",
    "# Method 2: Compute thickness from 'z' coordinate (depth)\n",
    "elif 'z' in ds.coords:\n",
    "    # Layer thickness = difference in z between adjacent layers\n",
    "    # For each layer, thickness = |z[i] - z[i+1]|, except last layer\n",
    "    z = ds.coords['z']\n",
    "    \n",
    "    # Compute thickness by differencing z (absolute value since z is negative)\n",
    "    # This is a simplified approach - in reality, you'd need to handle the last layer carefully\n",
    "    z_diff = z.diff(dim='layer', label='upper')\n",
    "    thickness = -z_diff  # Negative because z decreases downward\n",
    "    \n",
    "    # For the last layer, we need to estimate thickness\n",
    "    # This is a simplified calculation\n",
    "    \n",
    "    # Alternative: If HS (total height) is available, we can use it\n",
    "    if 'HS' in ds.data_vars:\n",
    "        # Approximate: assume layers are evenly distributed\n",
    "        # This is not perfect but gives an idea\n",
    "        # For demonstration, we'll show the concept\n",
    "        swe_approx = (ds['density'] * ds['HS'] / ds.dims['layer']).sum(dim='layer') / 1000.0\n",
    "\n",
    "# Method 3: If SWE is already in the dataset\n",
    "if 'SWE' in ds.data_vars or 'swe' in ds.data_vars:\n",
    "    swe_var = ds['SWE'] if 'SWE' in ds.data_vars else ds['swe']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** SWE estimates depend on which supporting variables exist, so the notebook demonstrates fallback strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: SWE placeholders\n",
    "if 'SWE' in ds.data_vars:\n",
    "    assert swe_var.dims == ds['SWE'].dims\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Identifying Weak Layers\n",
    "**Show.** Combine density and grain clues to flag weak layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run.\n",
    "# Define weak layer criteria\n",
    "# This is a simplified example - real weak layer identification is more complex\n",
    "\n",
    "weak_mask = None\n",
    "\n",
    "if 'density' in ds.data_vars:\n",
    "    # Weak layers often have low density (< 150 kg/m\u00b3)\n",
    "    low_density = ds['density'] < 150\n",
    "    \n",
    "    if 'grain_type' in ds.data_vars:\n",
    "        # Some grain types indicate weak layers (this depends on your grain type coding)\n",
    "        # Example: grain_type == 4 might indicate faceted crystals\n",
    "        # Note: Check your data's grain type coding scheme!\n",
    "        # weak_grain = ds['grain_type'] == 4  # Example only\n",
    "        weak_mask = low_density  # Simplified: just use density for now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** Even simple heuristics (like low density) can highlight suspect layers for deeper investigation.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: weak mask created\n",
    "assert weak_mask is not None\n",
    "assert weak_mask.dims == ds['density'].dims\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Time Series Operations\n",
    "**Show.** Analyze change over time by averaging, slicing, and differencing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run.\n",
    "# Compute mean over time (average profile)\n",
    "if 'density' in ds.data_vars:\n",
    "    mean_density_over_time = ds['density'].mean(dim='time')\n",
    "\n",
    "# Compute time series of profile-level variables\n",
    "if 'HS' in ds.data_vars:\n",
    "    hs_series = ds['HS'].isel(location=0, slope=0, realization=0)\n",
    "\n",
    "# Compute change over time (difference between consecutive time steps)\n",
    "if 'HS' in ds.data_vars:\n",
    "    hs_change = ds['HS'].diff(dim='time')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** Time-based reductions and differences expose evolving snowpack structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: time operations\n",
    "if 'density' in ds.data_vars:\n",
    "    assert 'layer' in mean_density_over_time.dims\n",
    "if 'HS' in ds.data_vars:\n",
    "    assert hs_change.dims['time'] == ds.dims['time'] - 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Values as NumPy Arrays\n",
    "**Show.** Convert xarray-backed data into bare NumPy arrays for external tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run.\n",
    "# Get values as NumPy array\n",
    "density_array = ds['density'].values\n",
    "\n",
    "# Get values for a specific selection\n",
    "surface_density = ds['density'].sel(layer=0).values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** Pulling NumPy arrays lets you hand data to scikit-learn, SciPy, or custom simulations.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: numpy access\n",
    "assert density_array.ndim >= 1\n",
    "assert surface_density.shape[0] == ds.dims['time']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play\n",
    "Adjust the density threshold or depth window to explore how selections change. Keep runs lightweight by sampling a single location.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run.\n",
    "density_cutoff = 280  # Try between 250 and 320\n",
    "depth_limit = -0.6  # Try between -0.2 and -1.0\n",
    "\n",
    "subset = ds.isel(location=0).where(ds['density'] > density_cutoff, drop=True)\n",
    "shallow_subset = subset.where(ds.coords['z'] >= depth_limit, drop=True)\n",
    "print('Layers retained:', shallow_subset.dims.get('layer', 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "Test yourself with these prompts before opening the solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a mask for layers where temperature is warmer than -5\u00b0C and inspect the remaining density.\n",
    "2. Compute the rolling 3-step mean of `HS` for a single location.\n",
    "3. Export a selection of `density` to a pandas DataFrame and describe the index levels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solutions</summary>\n",
    "\n",
    "1. `warm = ds.where(ds['temperature'] > -5, drop=True)` then inspect `warm['density']`.\n",
    "2. `ds['HS'].isel(location=0, slope=0, realization=0).rolling(time=3).mean()` demonstrates smoothing.\n",
    "3. `ds['density'].isel(location=0).to_dataframe().head()` reveals the MultiIndex structure.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Coordinate-based selections and boolean masks isolate layers for closer study.\n",
    "- Aggregations and SWE estimates condense stratigraphy into actionable metrics.\n",
    "- Exporting to NumPy unlocks interoperability with broader scientific tools.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}