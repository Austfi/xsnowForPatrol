{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05: Working with Custom Data\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Austfi/xsnowForPatrol/blob/main/notebooks/05_working_with_custom_data.ipynb)\n",
    "\n",
    "This notebook shows you how to prepare and load your own SNOWPACK output files into xsnow.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Preparing your own .pro and .smet files\n",
    "- File format requirements\n",
    "- Loading custom data\n",
    "- Troubleshooting common issues\n",
    "- Merging multiple data sources\n",
    "- Data validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning objectives\n",
    "- Review xsnow file expectations so your custom data loads cleanly.\n",
    "- Practice validating file paths, headers, and required variables.\n",
    "- Load one or many files with `xsnow.read` while trapping common errors.\n",
    "- Merge profile and meteorological datasets to build a richer context.\n",
    "\n",
    "**Prerequisites**\n",
    "- [ ] Comfortable with notebooks 01\u201304.\n",
    "- [ ] Basic familiarity with filesystem paths.\n",
    "- [ ] Ability to interpret snow profile variables in xsnow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation (For Colab Users)\n",
    "**Show.** Install xsnow and scientific dependencies when running remotely.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run.\n",
    "%pip install -q numpy pandas xarray matplotlib seaborn dask netcdf4\n",
    "%pip install -q git+https://gitlab.com/avacollabra/postprocessing/xsnow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Reference Dataset\n",
    "**Show.** Load the sample dataset so you always have a known-good structure for validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run.\n",
    "import xsnow\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "print('Loading xsnow reference dataset...')\n",
    "reference_ds = xsnow.single_profile_timeseries()\n",
    "print('\u2705 Reference dataset dims:', dict(reference_ds.dims))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** A working dataset gives you something to compare against when debugging custom files.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: reference dataset\n",
    "assert reference_ds is not None\n",
    "assert 'location' in reference_ds.dims\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: File Format Requirements\n",
    "**Show.** Summarize the suffixes and metadata xsnow expects.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run.\n",
    "required_suffixes = {'.pro', '.smet'}\n",
    "print('Accepted profile suffixes:', required_suffixes)\n",
    "print('Required coordinate names:', ['time', 'location', 'layer'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** Confirming file types early prevents silent parsing failures later.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: suffix set\n",
    "assert '.pro' in required_suffixes\n",
    "assert '.smet' in required_suffixes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Preparing Your Files\n",
    "**Show.** Check directories and filenames before attempting to load them.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run.\n",
    "data_dir = Path('data')\n",
    "pro_files = sorted(data_dir.glob('*.pro'))\n",
    "smet_files = sorted(data_dir.glob('*.smet'))\n",
    "print('Found profile files:', [p.name for p in pro_files])\n",
    "print('Found meteo files:', [p.name for p in smet_files])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** Verifying file presence and extension catches typos before they trigger stack traces.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: file listing\n",
    "assert isinstance(pro_files, list)\n",
    "assert isinstance(smet_files, list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Loading Your Custom Data\n",
    "**Show.** Use `xsnow.read` to open single files or collections.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run.\n",
    "example_single = data_dir / 'example.pro'\n",
    "example_many = [data_dir / 'station1.pro', data_dir / 'station2.pro']\n",
    "print('Single-file pattern:', example_single)\n",
    "print('Multi-file pattern:', example_many)\n",
    "# For demonstration, reuse the reference dataset\n",
    "custom_ds = reference_ds\n",
    "print('Placeholder dataset dimensions:', dict(custom_ds.dims))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** Passing either a path or list gives xsnow the context it needs to assemble a dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: dataset placeholder\n",
    "assert custom_ds is reference_ds\n",
    "assert isinstance(custom_ds.dims, dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Multiple Files\n",
    "**Show.** Build file lists programmatically to avoid manual errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run.\n",
    "station_codes = ['station1', 'station2', 'station3']\n",
    "pattern = [data_dir / f'{code}.pro' for code in station_codes]\n",
    "print('Pattern list:', pattern)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** Generating lists keeps your workflow reproducible and shareable.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: pattern list\n",
    "assert len(pattern) == 3\n",
    "assert all(p.suffix == '.pro' for p in pattern)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Troubleshooting Common Issues\n",
    "**Show.** Guard against missing files and unexpected headers.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run.\n",
    "def file_exists(path: Path) -> bool:\n",
    "    exists = path.exists()\n",
    "    if not exists:\n",
    "        print(f'\u26a0\ufe0f Missing file: {path}')\n",
    "    return exists\n",
    "\n",
    "example_path = data_dir / 'station1.pro'\n",
    "file_exists(example_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** Checking for existence first avoids confusing stack traces from deep within xsnow.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: existence helper\n",
    "assert callable(file_exists)\n",
    "assert isinstance(file_exists(example_path), bool)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Headers\n",
    "**Show.** Peek at the first few lines to confirm formatting.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run.\n",
    "try:\n",
    "    header_preview = example_path.read_text().splitlines()[:5]\n",
    "except FileNotFoundError:\n",
    "    header_preview = ['# Example SMET header', 'fields = time HS TA']\n",
    "print('Header preview:', header_preview)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** A quick glance reveals whether required fields like `time` or `HS` are present.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: header preview\n",
    "assert isinstance(header_preview, list)\n",
    "assert len(header_preview) > 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Variables\n",
    "**Show.** Confirm that required variables made it into the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run.\n",
    "required_vars = {'HS', 'density'}\n",
    "missing = [var for var in required_vars if var not in custom_ds.data_vars]\n",
    "print('Missing variables:', missing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** Comparing against a required set keeps you from analyzing incomplete data.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: variable coverage\n",
    "assert isinstance(missing, list)\n",
    "assert required_vars == {'HS', 'density'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Alignment\n",
    "**Show.** Verify that timestamps are monotonic and unique.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run.\n",
    "times = custom_ds['time'].values\n",
    "is_sorted = (times[1:] >= times[:-1]).all() if len(times) > 1 else True\n",
    "print('Times monotonic:', bool(is_sorted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** Misordered timestamps can break merges and rolling calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: time monotonicity\n",
    "assert isinstance(is_sorted, (bool, np.bool_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Data Validation\n",
    "**Show.** Run lightweight checks before trusting analysis outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run.\n",
    "def validate_dataset(ds):\n",
    "    issues = []\n",
    "    if ds.isnull().any():\n",
    "        issues.append('Contains NaNs')\n",
    "    if (ds['HS'] < 0).any():\n",
    "        issues.append('Negative snow height detected')\n",
    "    return issues\n",
    "\n",
    "validation_messages = validate_dataset(custom_ds)\n",
    "print('Validation issues:', validation_messages or 'None detected')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** Automated checks keep datasets healthy as new files roll in.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: validation helper\n",
    "assert callable(validate_dataset)\n",
    "assert isinstance(validation_messages, list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Merging Profile and Meteorological Data\n",
    "**Show.** Combine complementary datasets for context.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run.\n",
    "profile_subset = reference_ds[['HS', 'density']]\n",
    "meteo_like = reference_ds[['TA']] if 'TA' in reference_ds.data_vars else reference_ds[['HS']]\n",
    "merged = profile_subset.merge(meteo_like, join='inner')\n",
    "print('Merged variables:', list(merged.data_vars))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain.** Merging ensures snow structure and weather live in the same dataset for downstream models.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for understanding: merge result\n",
    "assert set(profile_subset.dims).issuperset(set(merged.dims))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play\n",
    "Experiment with different glob patterns or required variable sets to match your project.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run.\n",
    "glob_pattern = '*.pro'  # Try '*.smet' or '**/*.pro'\n",
    "required_vars_play = {'HS', 'temperature'}\n",
    "files = sorted(data_dir.glob(glob_pattern))\n",
    "print('Matched files:', [f.name for f in files])\n",
    "print('Required vars to check:', required_vars_play)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "Challenge yourself before reading the answers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a helper that validates coordinate names before calling `xsnow.read`.\n",
    "2. Create a summary table counting how many files each location contributes.\n",
    "3. Draft an error message template for missing required variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solutions</summary>\n",
    "\n",
    "1. Check `path.name` against expected tokens before calling `xsnow.read`.\n",
    "2. After loading, use `ds.groupby('location').size()` to count profiles per site.\n",
    "3. Use something like `f\"Required variables missing: {missing}\"` to guide the next steps.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Validate filenames, headers, and variables before loading custom xsnow data.\n",
    "- `xsnow.read` handles single or multiple files when paths are organized.\n",
    "- Small helper functions catch issues early and streamline future ingests.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}