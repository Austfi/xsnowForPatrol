{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05: Working with Custom Data\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Austfi/xsnowForPatrol/blob/main/notebooks/05_working_with_custom_data.ipynb)\n",
    "\n",
    "This notebook shows you how to prepare and load your own SNOWPACK output files into xsnow.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Preparing your own .pro and .smet files\n",
    "- File format requirements\n",
    "- Loading custom data\n",
    "- Troubleshooting common issues\n",
    "- Merging multiple data sources\n",
    "- Basic data validation\n",
    "\n",
    "> **Note**: For comprehensive missing data handling, see **07_data_quality_and_cleaning.ipynb**. For performance optimization and Zarr format, see **09_performance_and_storage.ipynb**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation (For Colab Users)\n",
    "\n",
    "If you're using Google Colab, run the cell below to install xsnow and dependencies. If you're running locally and have already installed xsnow, you can skip this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install -q numpy pandas xarray matplotlib seaborn dask netcdf4 zarr\n",
    "%pip install -q git+https://gitlab.com/avacollabra/postprocessing/xsnow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xsnow\n",
    "import os\n",
    "import glob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Explore xsnow sample data\n",
    "import xsnow\n",
    "\n",
    "print(\"xsnow provides sample datasets:\")\n",
    "print()\n",
    "\n",
    "# Example 1: Single profile\n",
    "print(\"1. Single profile (one snapshot):\")\n",
    "ds_single = xsnow.single_profile()\n",
    "print(f\"   Dimensions: {dict(ds_single.dims)}\")\n",
    "\n",
    "print()\n",
    "# Example 2: Time series\n",
    "print(\"2. Time series (multiple snapshots over time):\")\n",
    "ds_timeseries = xsnow.single_profile_timeseries()\n",
    "print(f\"   Dimensions: {dict(ds_timeseries.dims)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: File Format Requirements\n",
    "\n",
    "xsnow can read SNOWPACK output files in these formats:\n",
    "\n",
    "### .pro Files (Profile Time Series)\n",
    "\n",
    "- **Format**: SNOWPACK profile format (legacy)\n",
    "- **Contains**: Time series of snow profiles with layer-by-layer data\n",
    "- **Required**: Header with station metadata, profile data blocks\n",
    "- **Generated by**: SNOWPACK when `PROF_FORMAT = PRO` in .ini file\n",
    "\n",
    "### .smet Files (Meteorological Time Series)\n",
    "\n",
    "- **Format**: SMET (MeteoIO format)\n",
    "- **Contains**: Time series of scalar variables (no layers)\n",
    "- **Required**: SMET header with field descriptions, time series data\n",
    "- **Generated by**: SNOWPACK or MeteoIO for meteorological data\n",
    "\n",
    "### Other Formats\n",
    "\n",
    "xsnow may support other formats (check documentation):\n",
    "- NetCDF (if SNOWPACK outputs to NetCDF)\n",
    "- Other SNOWPACK output formats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Preparing Your Files\n",
    "\n",
    "### Step 1: Generate SNOWPACK Output\n",
    "\n",
    "If you're running SNOWPACK yourself:\n",
    "\n",
    "1. **Configure SNOWPACK** (via Inishell or .ini file):\n",
    "   - Set `PROF_FORMAT = PRO` to generate .pro files\n",
    "   - Configure which variables to output\n",
    "   - Set output directory\n",
    "\n",
    "2. **Run SNOWPACK** simulation\n",
    "\n",
    "3. **Check output files**:\n",
    "   - Look for `.pro` files in output directory\n",
    "   - Check for `.smet` files if configured\n",
    "\n",
    "### Step 2: Verify File Format\n",
    "\n",
    "Let's check if your files are in the correct format:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for .pro files in data directory\n",
    "data_dir = \"data\"\n",
    "pro_files = glob.glob(os.path.join(data_dir, \"*.pro\"))\n",
    "smet_files = glob.glob(os.path.join(data_dir, \"*.smet\"))\n",
    "\n",
    "print(f\"Found {len(pro_files)} .pro files:\")\n",
    "for f in pro_files[:5]:  # Show first 5\n",
    "    print(f\"  {f}\")\n",
    "\n",
    "print(f\"\\nFound {len(smet_files)} .smet files:\")\n",
    "for f in smet_files[:5]:  # Show first 5\n",
    "    print(f\"  {f}\")\n",
    "\n",
    "# Quick format check\n",
    "if pro_files:\n",
    "    first_file = pro_files[0]\n",
    "    print(f\"\\nInspecting first .pro file: {first_file}\")\n",
    "    with open(first_file, 'r') as f:\n",
    "        first_lines = [f.readline() for _ in range(10)]\n",
    "        print(\"First 5 non-empty lines:\")\n",
    "        for i, line in enumerate(first_lines[:5]):\n",
    "            if line.strip():  # Only show non-empty lines\n",
    "                print(f\"  Line {i+1}: {line.strip()[:80]}\")\n",
    "else:\n",
    "    print(\"\\nNo .pro files found in data directory.\")\n",
    "    print(\"Note: You can use xsnow's built-in sample data instead!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Loading Your Custom Data\n",
    "\n",
    "Now let's load your files:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now You Try**: After checking for files, try:\n",
    "- Listing all files in a different directory\n",
    "- Checking file sizes to see which files are largest\n",
    "- Using `os.path.getmtime()` to find the most recently modified file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Load a single file\n",
    "# Uncomment and modify path to load your own file:\n",
    "# ds = xsnow.read(\"data/your_file.pro\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Multiple Files\n",
    "\n",
    "You can load multiple files at once:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now You Try**: After loading a file, try:\n",
    "- Loading a file from a different directory (use an absolute path)\n",
    "- Loading multiple files and comparing their dimensions\n",
    "- Inspecting the first few rows of data after loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Load multiple files\n",
    "# List of files\n",
    "# ds = xsnow.read(['data/file1.pro', 'data/file2.pro'])\n",
    "\n",
    "# All files in directory\n",
    "# ds = xsnow.read('data/')\n",
    "\n",
    "# Mix of .pro and .smet\n",
    "# ds = xsnow.read(['data/profile.pro', 'data/meteo.smet'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Troubleshooting Common Issues\n",
    "\n",
    "### Issue 1: File Not Found\n",
    "\n",
    "**Error**: `FileNotFoundError` or similar\n",
    "\n",
    "**Solutions**:\n",
    "- Check file path is correct\n",
    "- Use absolute paths if relative paths don't work\n",
    "- Verify file exists: `os.path.exists('path/to/file.pro')`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now You Try**: After troubleshooting file format issues, try:\n",
    "- Creating a function to validate file format before loading\n",
    "- Writing code to automatically detect file format (.pro vs .smet)\n",
    "- Checking if files have the expected header structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Check if file exists before loading\n",
    "# test_file = \"data/your_file.pro\"\n",
    "# if os.path.exists(test_file):\n",
    "#     ds = xsnow.read(test_file)\n",
    "# else:\n",
    "#     print(f\"File not found: {test_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue 2: Format Not Recognized\n",
    "\n",
    "**Error**: File format not supported or parsing errors\n",
    "\n",
    "**Solutions**:\n",
    "- Verify file is actual .pro or .smet format (not just renamed)\n",
    "- Check file header matches expected format\n",
    "- Try opening file in text editor to inspect structure\n",
    "- Check SNOWPACK version compatibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect file header\n",
    "# Uncomment to inspect your own file:\n",
    "# test_file = \"data/your_file.pro\"\n",
    "# with open(test_file, 'r') as f:\n",
    "#     header_lines = [f.readline().strip() for _ in range(20)]\n",
    "#     print(\"Header lines (first 20, non-empty):\")\n",
    "#     for i, line in enumerate(header_lines):\n",
    "#         if line:  # Skip empty lines\n",
    "#             print(f\"  {i+1}: {line[:100]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue 3: Missing Variables\n",
    "\n",
    "**Problem**: Expected variables not in dataset\n",
    "\n",
    "**Solutions**:\n",
    "- Check SNOWPACK output configuration\n",
    "- Verify variables were enabled in SNOWPACK .ini file\n",
    "- Some variables may be computed by xsnow (like HS, z)\n",
    "- Check variable names match xsnow's expected names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now You Try**: After validating your data, try:\n",
    "- Creating a summary report of data quality (count of NaNs, value ranges, etc.)\n",
    "- Writing a function to automatically validate multiple datasets\n",
    "- Comparing validation results between different files or locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available variables in your dataset\n",
    "# Uncomment after loading your data:\n",
    "# print(\"Available variables in dataset:\")\n",
    "# for var in list(ds.data_vars.keys())[:20]:\n",
    "#     print(f\"  {var}: {ds[var].dims}\")\n",
    "\n",
    "# Check for common variables\n",
    "# common_vars = ['density', 'temperature', 'HS', 'grain_type', 'grain_size']\n",
    "# for var in common_vars:\n",
    "#     if var in ds.data_vars:\n",
    "#         print(f\"  {var}: found\")\n",
    "#     else:\n",
    "#         print(f\"  {var}: not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue 4: Time Alignment Problems\n",
    "\n",
    "**Problem**: Multiple files have different time ranges or frequencies\n",
    "\n",
    "**Solutions**:\n",
    "- xsnow will try to align times automatically\n",
    "- Check time ranges: `ds.coords['time'].values`\n",
    "- Resample if needed: `ds.resample(time='1H').mean()`\n",
    "- Manually select overlapping time periods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check time range and frequency\n",
    "# Uncomment after loading your data:\n",
    "# times = ds.coords['time'].values\n",
    "# print(f\"Time range: {times[0]} to {times[-1]}\")\n",
    "# print(f\"Total time steps: {len(times)}\")\n",
    "# \n",
    "# if len(times) > 1:\n",
    "#     time_diff = times[1] - times[0]\n",
    "#     print(f\"Time step frequency: {time_diff}\")\n",
    "#     \n",
    "#     # Check if times are regular\n",
    "#     if len(times) > 2:\n",
    "#         time_diffs = np.diff(times)\n",
    "#         if np.allclose(time_diffs, time_diffs[0]):\n",
    "#             print(\"Times are regularly spaced\")\n",
    "#         else:\n",
    "#             print(\"Times are irregularly spaced\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now You Try**: After merging data, try:\n",
    "- Merging data from 3 or more files\n",
    "- Checking that merged data has the expected dimensions\n",
    "- Comparing variables before and after merging to ensure nothing was lost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Data Validation\n",
    "\n",
    "After loading, validate your data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check for NaN values\n",
    "# Uncomment after loading your data:\n",
    "# nan_count = ds['density'].isnull().sum().values\n",
    "# total_count = ds['density'].size\n",
    "# if nan_count > 0:\n",
    "#     print(f\"Found {nan_count} NaN values in density ({100*nan_count/total_count:.1f}%)\")\n",
    "# else:\n",
    "#     print(\"No NaN values in density\")\n",
    "\n",
    "# Check for reasonable value ranges\n",
    "# density_vals = ds['density'].values\n",
    "# valid_vals = density_vals[~np.isnan(density_vals)]\n",
    "# if len(valid_vals) > 0:\n",
    "#     print(f\"Density range: {valid_vals.min():.1f} to {valid_vals.max():.1f} kg/mÂ³\")\n",
    "#     if valid_vals.min() < 0 or valid_vals.max() > 1000:\n",
    "#         print(\"Density values outside typical range (0-1000 kg/mÂ³)\")\n",
    "#     else:\n",
    "#         print(\"Density values in reasonable range\")\n",
    "\n",
    "# temp_vals = ds['temperature'].values\n",
    "# valid_vals = temp_vals[~np.isnan(temp_vals)]\n",
    "# if len(valid_vals) > 0:\n",
    "#     print(f\"Temperature range: {valid_vals.min():.1f} to {valid_vals.max():.1f} Â°C\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Merging Profile and Meteorological Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note on Missing Data**: For comprehensive coverage of missing data detection, handling strategies (interpolation, filling, dropping), and cleaning pipelines, see **07_data_quality_and_cleaning.ipynb**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Check results\n",
    "print(f\"\\nOriginal missing: {ds['density'].isnull().sum().values}\")\n",
    "print(f\"After mean fill: {density_mean_filled.isnull().sum().values}\")\n",
    "print(f\"After median fill: {density_median_filled.isnull().sum().values}\")\n",
    "print(f\"After location mean fill: {density_location_filled.isnull().sum().values}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy 4: Dropping Missing Data\n",
    "\n",
    "Sometimes it's better to remove data with missing values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Drop missing data with different strategies\n",
    "\n",
    "# Strategy 1: Drop any layer with missing density\n",
    "density_no_missing_layers = ds['density'].dropna(dim='layer')\n",
    "print(f\"Drop missing layers: {density_no_missing_layers.dims.get('layer', 'N/A')} layers remaining\")\n",
    "\n",
    "# Strategy 2: Drop entire profiles with ANY missing values (aggressive)\n",
    "profiles_clean = ds.dropna(dim='layer', how='any')\n",
    "print(f\"Drop profiles with any missing: {profiles_clean.dims.get('time', 'N/A')} profiles remaining\")\n",
    "\n",
    "# Strategy 3: Drop only if ALL values are missing (conservative)\n",
    "profiles_partial = ds.dropna(dim='layer', how='all')\n",
    "print(f\"Drop only fully-missing layers: {profiles_partial.dims.get('time', 'N/A')} profiles remaining\")\n",
    "\n",
    "# Strategy 4: Drop specific time steps with missing data\n",
    "time_clean = ds.dropna(dim='time', how='any')\n",
    "print(f\"Drop time steps with any missing: {time_clean.dims.get('time', 'N/A')} time steps remaining\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy 5: Multi-Step Cleaning Pipeline\n",
    "\n",
    "For real-world data, combine multiple strategies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive cleaning pipeline for density data\n",
    "def clean_density_data(ds, variable='density'):\n",
    "    \"\"\"\n",
    "    Multi-step cleaning pipeline for density data.\n",
    "    \n",
    "    Steps:\n",
    "    1. Interpolate missing values along layer dimension\n",
    "    2. Forward/backward fill any remaining gaps\n",
    "    3. Fill with location-specific mean if still missing\n",
    "    4. Drop profiles with >50% missing data\n",
    "    \"\"\"\n",
    "    density = ds[variable].copy()\n",
    "    \n",
    "    # Step 1: Interpolate along layer dimension (for depth profiles)\n",
    "    density = density.interpolate_na(dim='layer', method='linear')\n",
    "    \n",
    "    # Step 2: Forward then backward fill\n",
    "    density = density.fillna(method='ffill', dim='layer')\n",
    "    density = density.fillna(method='bfill', dim='layer')\n",
    "    \n",
    "    # Step 3: Fill remaining with location-specific mean\n",
    "    location_mean = density.mean(dim=['time', 'layer', 'slope', 'realization'])\n",
    "    density = density.fillna(location_mean)\n",
    "    \n",
    "    # Step 4: Drop profiles with >50% missing (if any remain)\n",
    "    missing_per_profile = density.isnull().sum(dim='layer') / density.sizes['layer']\n",
    "    valid_profiles = missing_per_profile < 0.5\n",
    "    \n",
    "    # Apply filter (keep only valid profiles)\n",
    "    # Note: This is simplified - in practice you'd need to filter the dataset\n",
    "    \n",
    "    return density\n",
    "\n",
    "# Apply cleaning pipeline\n",
    "density_cleaned = clean_density_data(ds, 'density')\n",
    "print(\"Cleaning pipeline applied:\")\n",
    "print(f\"  Original missing: {ds['density'].isnull().sum().values}\")\n",
    "print(f\"  After cleaning: {density_cleaned.isnull().sum().values}\")\n",
    "print(f\"  Missing values removed: {ds['density'].isnull().sum().values - density_cleaned.isnull().sum().values}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Right Strategy\n",
    "\n",
    "**Use interpolation when:**\n",
    "- Missing values are in time series data\n",
    "- You have enough neighboring data points\n",
    "- Values change smoothly over time/depth\n",
    "\n",
    "**Use forward/backward fill when:**\n",
    "- Missing values are at edges (surface or bottom layers)\n",
    "- Values are relatively constant\n",
    "- Quick fix needed\n",
    "\n",
    "**Use statistical imputation when:**\n",
    "- Missing values are scattered randomly\n",
    "- You have good overall statistics\n",
    "- Interpolation isn't appropriate\n",
    "\n",
    "**Use dropping when:**\n",
    "- Missing data is extensive (>50% of profile)\n",
    "- Missing data indicates data quality issues\n",
    "- You have enough remaining data for analysis\n",
    "\n",
    "**Use multi-step pipeline when:**\n",
    "- Data has complex missing patterns\n",
    "- You need robust cleaning\n",
    "- Production data processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now You Try**: After learning about missing data handling, try:\n",
    "- Creating a validation report for your dataset showing missing data patterns\n",
    "- Applying different cleaning strategies and comparing results\n",
    "- Creating a custom cleaning function for your specific data needs\n",
    "- Validating that cleaned data produces reasonable analysis results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5.5: Data Type Optimization\n",
    "\n",
    "For large datasets, optimizing data types can significantly reduce memory usage and improve performance. Understanding and choosing appropriate data types (dtypes) is crucial for efficient data processing.\n",
    "\n",
    "**What you'll see**: The examples below show how to check, convert, and optimize data types to reduce memory usage while maintaining data quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Data Types\n",
    "\n",
    "Different data types use different amounts of memory:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current data types\n",
    "print(\"Current data types in dataset:\")\n",
    "for var in list(ds.data_vars.keys())[:5]:\n",
    "    dtype = ds[var].dtype\n",
    "    print(f\"  {var}: {dtype}\")\n",
    "\n",
    "# Check memory usage\n",
    "import sys\n",
    "density_size = ds['density'].nbytes / (1024**2)  # Size in MB\n",
    "print(f\"\\nDensity array size: {density_size:.2f} MB\")\n",
    "\n",
    "# Common data types and their memory usage:\n",
    "print(\"\\nCommon NumPy data types:\")\n",
    "print(\"  int8:  1 byte  (range: -128 to 127)\")\n",
    "print(\"  int16: 2 bytes (range: -32,768 to 32,767)\")\n",
    "print(\"  int32: 4 bytes (range: -2.1B to 2.1B)\")\n",
    "print(\"  int64: 8 bytes (range: very large)\")\n",
    "print(\"  float32: 4 bytes (single precision, ~7 decimal digits)\")\n",
    "print(\"  float64: 8 bytes (double precision, ~15 decimal digits)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Data Types\n",
    "\n",
    "Convert data to more memory-efficient types when appropriate:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Convert float64 to float32 (halves memory usage)\n",
    "# Check if values fit in float32 range\n",
    "density_vals = ds['density'].values\n",
    "density_min = density_vals.min()\n",
    "density_max = density_vals.max()\n",
    "\n",
    "print(f\"Original dtype: {ds['density'].dtype}\")\n",
    "print(f\"Value range: {density_min:.1f} to {density_max:.1f} kg/mÂ³\")\n",
    "print(f\"Original size: {ds['density'].nbytes / (1024**2):.2f} MB\")\n",
    "\n",
    "# Convert to float32 (if values fit)\n",
    "if density_min >= np.finfo(np.float32).min and density_max <= np.finfo(np.float32).max:\n",
    "    density_float32 = ds['density'].astype('float32')\n",
    "    print(f\"Converted to float32: {density_float32.dtype}\")\n",
    "    print(f\"New size: {density_float32.nbytes / (1024**2):.2f} MB\")\n",
    "    print(f\"Memory saved: {(ds['density'].nbytes - density_float32.nbytes) / (1024**2):.2f} MB\")\n",
    "    print(f\"  (50% reduction for this variable)\")\n",
    "else:\n",
    "    print(\"Values outside float32 range - cannot convert safely\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Optimization Strategies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Convert all float64 to float32 (if precision allows)\n",
    "def optimize_dtypes(ds):\n",
    "    \"\"\"Convert data types to more memory-efficient types.\"\"\"\n",
    "    ds_optimized = ds.copy()\n",
    "    \n",
    "    for var in ds.data_vars:\n",
    "        if ds[var].dtype == 'float64':\n",
    "            # Check if values fit in float32\n",
    "            vals = ds[var].values\n",
    "            if np.isfinite(vals).all():  # Check for inf/nan\n",
    "                vals_finite = vals[np.isfinite(vals)]\n",
    "                if len(vals_finite) > 0:\n",
    "                    min_val = vals_finite.min()\n",
    "                    max_val = vals_finite.max()\n",
    "                    if (min_val >= np.finfo(np.float32).min and \n",
    "                        max_val <= np.finfo(np.float32).max):\n",
    "                        ds_optimized[var] = ds[var].astype('float32')\n",
    "                        print(f\"  {var}: float64 -> float32\")\n",
    "    \n",
    "    return ds_optimized\n",
    "\n",
    "# Apply optimization\n",
    "ds_optimized = optimize_dtypes(ds)\n",
    "\n",
    "# Compare memory usage\n",
    "original_size = sum(var.nbytes for var in ds.data_vars.values()) / (1024**2)\n",
    "optimized_size = sum(var.nbytes for var in ds_optimized.data_vars.values()) / (1024**2)\n",
    "print(f\"\\nOriginal dataset size: {original_size:.2f} MB\")\n",
    "print(f\"Optimized dataset size: {optimized_size:.2f} MB\")\n",
    "print(f\"Memory saved: {original_size - optimized_size:.2f} MB ({100*(original_size-optimized_size)/original_size:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Different Data Types\n",
    "\n",
    "**Use float32 when:**\n",
    "- Values fit in range (~-3.4e38 to 3.4e38)\n",
    "- ~7 decimal digits precision is sufficient\n",
    "- Memory is limited\n",
    "- Working with large datasets\n",
    "\n",
    "**Use float64 when:**\n",
    "- High precision is required (~15 decimal digits)\n",
    "- Values might be very large or very small\n",
    "- Scientific calculations need maximum precision\n",
    "- Memory is not a concern\n",
    "\n",
    "**Use int types when:**\n",
    "- Data is integer-valued (e.g., layer counts, indices)\n",
    "- Can use int16 or int32 instead of int64\n",
    "- Significant memory savings possible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Example: Optimizing Large Time Series\n",
    "\n",
    "For large time series datasets, dtype optimization can save significant memory:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Optimize a large time series dataset\n",
    "# Simulate a large dataset (many time steps, locations, layers)\n",
    "print(\"Example: Optimizing large time series dataset\")\n",
    "\n",
    "# Check current memory usage\n",
    "total_memory = 0\n",
    "for var_name, var in ds.data_vars.items():\n",
    "    var_memory = var.nbytes / (1024**2)\n",
    "    total_memory += var_memory\n",
    "    if var_memory > 0.1:  # Show variables using >0.1 MB\n",
    "        print(f\"  {var_name}: {var_memory:.2f} MB ({var.dtype})\")\n",
    "\n",
    "print(f\"\\nTotal dataset memory: {total_memory:.2f} MB\")\n",
    "\n",
    "# Optimize: Convert float64 to float32\n",
    "ds_opt = ds.copy()\n",
    "for var_name in ds.data_vars:\n",
    "    if ds[var_name].dtype == 'float64':\n",
    "        try:\n",
    "            ds_opt[var_name] = ds[var_name].astype('float32')\n",
    "        except (ValueError, OverflowError):\n",
    "            print(f\"  Cannot convert {var_name} to float32 (values out of range)\")\n",
    "\n",
    "# Check optimized memory\n",
    "total_memory_opt = sum(var.nbytes for var in ds_opt.data_vars.values()) / (1024**2)\n",
    "print(f\"\\nOptimized dataset memory: {total_memory_opt:.2f} MB\")\n",
    "print(f\"Memory saved: {total_memory - total_memory_opt:.2f} MB\")\n",
    "print(f\"Reduction: {100*(total_memory - total_memory_opt)/total_memory:.1f}%\")\n",
    "\n",
    "# For very large datasets, this can save gigabytes!\n",
    "print(\"\\nðŸ’¡ Tip: For datasets with 1000s of time steps and locations,\")\n",
    "print(\"   dtype optimization can save significant memory and improve performance.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trade-offs: Precision vs Memory\n",
    "\n",
    "**Important considerations:**\n",
    "- **Precision loss**: float32 has ~7 decimal digits vs float64's ~15\n",
    "- **Range limits**: float32 range is smaller than float64\n",
    "- **Performance**: float32 can be faster on some systems\n",
    "- **Compatibility**: Some operations may require float64\n",
    "\n",
    "**Best practice**: \n",
    "- Use float32 for storage and visualization\n",
    "- Use float64 for critical calculations\n",
    "- Test that precision loss doesn't affect your analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now You Try**: After learning about data type optimization, try:\n",
    "- Checking the data types and memory usage of your dataset\n",
    "- Converting appropriate variables to float32 and measuring memory savings\n",
    "- Creating a function to automatically optimize data types for your datasets\n",
    "- Comparing analysis results before and after dtype optimization to ensure precision is sufficient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Merge profile and meteo data\n",
    "# Load both at once\n",
    "# ds = xsnow.read(['data/profile.pro', 'data/meteo.smet'])\n",
    "\n",
    "# Or load separately and merge\n",
    "# ds_pro = xsnow.read('data/profile.pro')\n",
    "# ds_met = xsnow.read('data/meteo.smet')\n",
    "# ds_combined = xr.merge([ds_pro, ds_met])  # Using xarray's merge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Working with Zarr Format for Large Datasets\n",
    "\n",
    "Since xsnow is built on xarray, it can work with Zarr-backed datasets. **Zarr** is a format for storing chunked, compressed, N-dimensional arrays, which is ideal for large snowpack datasets.\n",
    "\n",
    "### What is Zarr?\n",
    "\n",
    "**Zarr** is a storage format that:\n",
    "- Stores data in **chunks** (smaller pieces) rather than one large file\n",
    "- Supports **compression** to reduce storage size\n",
    "- Enables **lazy loading** (only load what you need)\n",
    "- Works well with **Dask** for parallel computing\n",
    "- Is efficient for **large time series** or **multiple locations**\n",
    "\n",
    "### When to Use Zarr\n",
    "\n",
    "Consider using Zarr when you have:\n",
    "- **Large time series**: Many years of hourly/daily data\n",
    "- **Multiple locations**: Data from many stations or grid points\n",
    "- **Ensemble runs**: Multiple realizations or scenarios\n",
    "- **Limited memory**: Need to work with data larger than RAM\n",
    "- **Cloud storage**: Want to store data in cloud object storage (S3, GCS, etc.)\n",
    "\n",
    "### Benefits for Snowpack Data\n",
    "\n",
    "- **Efficient storage**: Compressed chunks reduce file size\n",
    "- **Fast access**: Load only the chunks you need\n",
    "- **Parallel processing**: Works seamlessly with Dask\n",
    "- **Scalable**: Handle datasets that don't fit in memory\n",
    "\n",
    "### Converting to Zarr Format\n",
    "\n",
    "**What you'll see**: The examples below show how to save xsnow datasets to Zarr format and load them back.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import zarr\n",
    "import xarray as xr\n",
    "\n",
    "# Example 1: Save dataset to Zarr format\n",
    "# Load sample data first\n",
    "ds = xsnow.single_profile_timeseries()\n",
    "\n",
    "# Save to Zarr (chunked and compressed)\n",
    "zarr_path = \"snowpack_data.zarr\"\n",
    "print(f\"Saving dataset to Zarr format: {zarr_path}\")\n",
    "\n",
    "# Configure chunking strategy\n",
    "# Chunk by time and location for efficient access\n",
    "chunks = {\n",
    "    'time': 100,      # 100 time steps per chunk\n",
    "    'location': 1,    # 1 location per chunk\n",
    "    'layer': -1,      # All layers in one chunk\n",
    "    'slope': -1,      # All slopes in one chunk\n",
    "    'realization': -1 # All realizations in one chunk\n",
    "}\n",
    "\n",
    "# Save to Zarr with compression\n",
    "try:\n",
    "    ds.to_zarr(\n",
    "        zarr_path,\n",
    "        mode='w',  # 'w' = write (overwrite), 'a' = append\n",
    "        encoding={\n",
    "            'density': {'compressor': zarr.Blosc(cname='zstd', clevel=3)},\n",
    "            'temperature': {'compressor': zarr.Blosc(cname='zstd', clevel=3)},\n",
    "        }\n",
    "    )\n",
    "    print(f\"âœ… Saved to {zarr_path}\")\n",
    "    \n",
    "    # Check file size\n",
    "    import os\n",
    "    if os.path.exists(zarr_path):\n",
    "        # Zarr creates a directory, so we need to check its size\n",
    "        total_size = sum(\n",
    "            os.path.getsize(os.path.join(dirpath, filename))\n",
    "            for dirpath, dirnames, filenames in os.walk(zarr_path)\n",
    "            for filename in filenames\n",
    "        )\n",
    "        print(f\"   Zarr store size: {total_size / 1024 / 1024:.2f} MB\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Zarr save example (may need actual data): {e}\")\n",
    "\n",
    "# Example 2: Load from Zarr format\n",
    "# This enables lazy loading - data isn't loaded until you access it\n",
    "try:\n",
    "    ds_zarr = xr.open_zarr(zarr_path)\n",
    "    print(f\"\\nâœ… Loaded from Zarr: {zarr_path}\")\n",
    "    print(f\"   Dimensions: {dict(ds_zarr.dims)}\")\n",
    "    print(f\"   Data is lazy-loaded (not in memory yet)\")\n",
    "    \n",
    "    # Accessing data triggers loading\n",
    "    print(f\"\\n   Accessing a small subset...\")\n",
    "    sample = ds_zarr['density'].isel(location=0, time=0, layer=0).values\n",
    "    print(f\"   Sample value: {sample}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nNote: Zarr load example (file may not exist): {e}\")\n",
    "\n",
    "# Example 3: Working with large datasets using Dask\n",
    "# Zarr works seamlessly with Dask for parallel processing\n",
    "try:\n",
    "    # Open with Dask chunks for parallel processing\n",
    "    ds_dask = xr.open_zarr(zarr_path, chunks={'time': 50, 'location': 1})\n",
    "    print(f\"\\nâœ… Opened with Dask chunks\")\n",
    "    print(f\"   Chunks: {ds_dask.chunks}\")\n",
    "    print(f\"   Data type: {type(ds_dask['density'].data)}\")\n",
    "    print(f\"   (Dask array - computation is lazy and parallel)\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nNote: Dask example (file may not exist): {e}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key points:\")\n",
    "print(\"   - Zarr stores data in compressed chunks\")\n",
    "print(\"   - Enables lazy loading (load only what you need)\")\n",
    "print(\"   - Works with Dask for parallel processing\")\n",
    "print(\"   - Ideal for datasets larger than available RAM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking Strategy for Snowpack Data\n",
    "\n",
    "When saving to Zarr, choose chunk sizes based on how you'll access the data:\n",
    "\n",
    "- **Time-based access** (e.g., \"all data for January\"): Chunk by time\n",
    "- **Location-based access** (e.g., \"all data for Station A\"): Chunk by location\n",
    "- **Profile-based access** (e.g., \"complete profiles\"): Chunk by time and location together\n",
    "- **Layer-based access** (e.g., \"surface layer over time\"): Chunk by layer\n",
    "\n",
    "**Rule of thumb**: Chunk size should be 10-100 MB for optimal performance.\n",
    "\n",
    "### Zarr vs NetCDF\n",
    "\n",
    "| Feature | Zarr | NetCDF |\n",
    "|---------|------|--------|\n",
    "| **Chunking** | Native, flexible | Limited |\n",
    "| **Compression** | Multiple algorithms | Limited |\n",
    "| **Lazy loading** | Excellent | Good |\n",
    "| **Cloud storage** | Native support | Requires special setup |\n",
    "| **File format** | Directory of files | Single file |\n",
    "| **Best for** | Large datasets, cloud | Standard scientific data |\n",
    "\n",
    "**For snowpack data**: Use Zarr when you have large time series or multiple locations. Use NetCDF for standard-sized datasets or when compatibility is important.\n",
    "\n",
    "**Now You Try**: After learning about Zarr, try:\n",
    "- Saving a dataset to Zarr format with different chunk sizes\n",
    "- Loading from Zarr and comparing memory usage vs. loading from NetCDF\n",
    "- Experimenting with different compression levels to balance size vs. speed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "âœ… **What we learned:**\n",
    "\n",
    "1. **File formats**: .pro (profiles) and .smet (meteorological) files\n",
    "2. **Loading custom data**: Use `xsnow.read()` with your file paths\n",
    "3. **Multiple files**: Load lists of files or entire directories\n",
    "4. **Troubleshooting**: Common issues and solutions\n",
    "5. **Validation**: Check data quality and ranges\n",
    "6. **Merging**: Combine profile and meteo data\n",
    "7. **Zarr format**: Chunked, compressed storage for large datasets\n",
    "\n",
    "## Key Tips\n",
    "\n",
    "- **File paths**: Use absolute paths if relative paths cause issues\n",
    "- **Format verification**: Inspect file headers to ensure correct format\n",
    "- **Variable names**: Check xsnow documentation for expected variable names\n",
    "- **Time alignment**: xsnow handles this automatically when merging\n",
    "- **Data quality**: Always validate loaded data\n",
    "- **Large datasets**: Consider Zarr format for efficient storage and access\n",
    "- **Chunking**: Choose chunk sizes based on your access patterns\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you can load your own data:\n",
    "- Apply analysis techniques from previous notebooks\n",
    "- Create visualizations with your data\n",
    "- Or learn to extend xsnow: **06_extending_xsnow.ipynb**\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Load one of your own .pro files and inspect its structure\n",
    "2. Check for missing variables and verify data ranges\n",
    "3. Load multiple files and compare their time ranges\n",
    "4. Merge a .pro and .smet file if you have both\n",
    "5. Validate your data and identify any quality issues\n",
    "6. Save a dataset to Zarr format and compare file sizes with NetCDF\n",
    "7. Load from Zarr and experiment with different chunk sizes\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
