{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05: Working with Custom Data\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Austfi/xsnowForPatrol/blob/main/notebooks/05_working_with_custom_data.ipynb)\n",
    "\n",
    "This notebook shows you how to prepare and load your own SNOWPACK output files into xsnow.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Preparing your own .pro and .smet files\n",
    "- File format requirements\n",
    "- Loading custom data\n",
    "- Troubleshooting common issues\n",
    "- Merging multiple data sources\n",
    "- Basic data validation\n",
    "\n",
    "> **Note**: For comprehensive missing data handling, see **07_data_quality_and_cleaning.ipynb**. For performance optimization and Zarr format, see **09_performance_and_storage.ipynb**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation (For Colab Users)\n",
    "\n",
    "If you're using Google Colab, run the cell below to install xsnow and dependencies. If you're running locally and have already installed xsnow, you can skip this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install -q numpy pandas xarray matplotlib seaborn dask netcdf4 zarr\n",
    "%pip install -q git+https://gitlab.com/avacollabra/postprocessing/xsnow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xsnow\n",
    "import os\n",
    "import glob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Explore xsnow sample data\n",
    "import xsnow\n",
    "\n",
    "print(\"xsnow provides sample datasets:\")\n",
    "print()\n",
    "\n",
    "# Example 1: Single profile\n",
    "print(\"1. Single profile (one snapshot):\")\n",
    "ds_single = xsnow.single_profile()\n",
    "print(f\"   Dimensions: {dict(ds_single.dims)}\")\n",
    "\n",
    "print()\n",
    "# Example 2: Time series\n",
    "print(\"2. Time series (multiple snapshots over time):\")\n",
    "ds_timeseries = xsnow.single_profile_timeseries()\n",
    "print(f\"   Dimensions: {dict(ds_timeseries.dims)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: File Format Requirements\n",
    "\n",
    "xsnow can read SNOWPACK output files in these formats:\n",
    "\n",
    "### .pro Files (Profile Time Series)\n",
    "\n",
    "- **Format**: SNOWPACK profile format (legacy)\n",
    "- **Contains**: Time series of snow profiles with layer-by-layer data\n",
    "- **Required**: Header with station metadata, profile data blocks\n",
    "- **Generated by**: SNOWPACK when `PROF_FORMAT = PRO` in .ini file\n",
    "\n",
    "### .smet Files (Meteorological Time Series)\n",
    "\n",
    "- **Format**: SMET (MeteoIO format)\n",
    "- **Contains**: Time series of scalar variables (no layers)\n",
    "- **Required**: SMET header with field descriptions, time series data\n",
    "- **Generated by**: SNOWPACK or MeteoIO for meteorological data\n",
    "\n",
    "### Other Formats\n",
    "\n",
    "xsnow may support other formats (check documentation):\n",
    "- NetCDF (if SNOWPACK outputs to NetCDF)\n",
    "- Other SNOWPACK output formats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Preparing Your Files\n",
    "\n",
    "### Step 1: Generate SNOWPACK Output\n",
    "\n",
    "If you're running SNOWPACK yourself:\n",
    "\n",
    "1. **Configure SNOWPACK** (via Inishell or .ini file):\n",
    "   - Set `PROF_FORMAT = PRO` to generate .pro files\n",
    "   - Configure which variables to output\n",
    "   - Set output directory\n",
    "\n",
    "2. **Run SNOWPACK** simulation\n",
    "\n",
    "3. **Check output files**:\n",
    "   - Look for `.pro` files in output directory\n",
    "   - Check for `.smet` files if configured\n",
    "\n",
    "### Step 2: Verify File Format\n",
    "\n",
    "Let's check if your files are in the correct format:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for .pro files in data directory\n",
    "data_dir = \"data\"\n",
    "pro_files = glob.glob(os.path.join(data_dir, \"*.pro\"))\n",
    "smet_files = glob.glob(os.path.join(data_dir, \"*.smet\"))\n",
    "\n",
    "print(f\"Found {len(pro_files)} .pro files:\")\n",
    "for f in pro_files[:5]:  # Show first 5\n",
    "    print(f\"  {f}\")\n",
    "\n",
    "print(f\"\\nFound {len(smet_files)} .smet files:\")\n",
    "for f in smet_files[:5]:  # Show first 5\n",
    "    print(f\"  {f}\")\n",
    "\n",
    "# Quick format check\n",
    "if pro_files:\n",
    "    first_file = pro_files[0]\n",
    "    print(f\"\\nInspecting first .pro file: {first_file}\")\n",
    "    with open(first_file, 'r') as f:\n",
    "        first_lines = [f.readline() for _ in range(10)]\n",
    "        print(\"First 5 non-empty lines:\")\n",
    "        for i, line in enumerate(first_lines[:5]):\n",
    "            if line.strip():  # Only show non-empty lines\n",
    "                print(f\"  Line {i+1}: {line.strip()[:80]}\")\n",
    "else:\n",
    "    print(\"\\nNo .pro files found in data directory.\")\n",
    "    print(\"Note: You can use xsnow's built-in sample data instead!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Loading Your Custom Data\n",
    "\n",
    "Now let's load your files:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now You Try**: After checking for files, try:\n",
    "- Listing all files in a different directory\n",
    "- Checking file sizes to see which files are largest\n",
    "- Using `os.path.getmtime()` to find the most recently modified file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Load a single file\n",
    "# Uncomment and modify path to load your own file:\n",
    "# ds = xsnow.read(\"data/your_file.pro\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Multiple Files\n",
    "\n",
    "You can load multiple files at once:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now You Try**: After loading a file, try:\n",
    "- Loading a file from a different directory (use an absolute path)\n",
    "- Loading multiple files and comparing their dimensions\n",
    "- Inspecting the first few rows of data after loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Load multiple files\n",
    "# List of files\n",
    "# ds = xsnow.read(['data/file1.pro', 'data/file2.pro'])\n",
    "\n",
    "# All files in directory\n",
    "# ds = xsnow.read('data/')\n",
    "\n",
    "# Mix of .pro and .smet\n",
    "# ds = xsnow.read(['data/profile.pro', 'data/meteo.smet'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Troubleshooting Common Issues\n",
    "\n",
    "### Issue 1: File Not Found\n",
    "\n",
    "**Error**: `FileNotFoundError` or similar\n",
    "\n",
    "**Solutions**:\n",
    "- Check file path is correct\n",
    "- Use absolute paths if relative paths don't work\n",
    "- Verify file exists: `os.path.exists('path/to/file.pro')`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now You Try**: After troubleshooting file format issues, try:\n",
    "- Creating a function to validate file format before loading\n",
    "- Writing code to automatically detect file format (.pro vs .smet)\n",
    "- Checking if files have the expected header structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Check if file exists before loading\n",
    "# test_file = \"data/your_file.pro\"\n",
    "# if os.path.exists(test_file):\n",
    "#     ds = xsnow.read(test_file)\n",
    "# else:\n",
    "#     print(f\"File not found: {test_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue 2: Format Not Recognized\n",
    "\n",
    "**Error**: File format not supported or parsing errors\n",
    "\n",
    "**Solutions**:\n",
    "- Verify file is actual .pro or .smet format (not just renamed)\n",
    "- Check file header matches expected format\n",
    "- Try opening file in text editor to inspect structure\n",
    "- Check SNOWPACK version compatibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect file header\n",
    "# Uncomment to inspect your own file:\n",
    "# test_file = \"data/your_file.pro\"\n",
    "# with open(test_file, 'r') as f:\n",
    "#     header_lines = [f.readline().strip() for _ in range(20)]\n",
    "#     print(\"Header lines (first 20, non-empty):\")\n",
    "#     for i, line in enumerate(header_lines):\n",
    "#         if line:  # Skip empty lines\n",
    "#             print(f\"  {i+1}: {line[:100]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue 3: Missing Variables\n",
    "\n",
    "**Problem**: Expected variables not in dataset\n",
    "\n",
    "**Solutions**:\n",
    "- Check SNOWPACK output configuration\n",
    "- Verify variables were enabled in SNOWPACK .ini file\n",
    "- Some variables may be computed by xsnow (like HS, z)\n",
    "- Check variable names match xsnow's expected names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now You Try**: After validating your data, try:\n",
    "- Creating a summary report of data quality (count of NaNs, value ranges, etc.)\n",
    "- Writing a function to automatically validate multiple datasets\n",
    "- Comparing validation results between different files or locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available variables in your dataset\n",
    "# Uncomment after loading your data:\n",
    "# print(\"Available variables in dataset:\")\n",
    "# for var in list(ds.data_vars.keys())[:20]:\n",
    "#     print(f\"  {var}: {ds[var].dims}\")\n",
    "\n",
    "# Check for common variables\n",
    "# common_vars = ['density', 'temperature', 'HS', 'grain_type', 'grain_size']\n",
    "# for var in common_vars:\n",
    "#     if var in ds.data_vars:\n",
    "#         print(f\"  {var}: found\")\n",
    "#     else:\n",
    "#         print(f\"  {var}: not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue 4: Time Alignment Problems\n",
    "\n",
    "**Problem**: Multiple files have different time ranges or frequencies\n",
    "\n",
    "**Solutions**:\n",
    "- xsnow will try to align times automatically\n",
    "- Check time ranges: `ds.coords['time'].values`\n",
    "- Resample if needed: `ds.resample(time='1H').mean()`\n",
    "- Manually select overlapping time periods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check time range and frequency\n",
    "# Uncomment after loading your data:\n",
    "# times = ds.coords['time'].values\n",
    "# print(f\"Time range: {times[0]} to {times[-1]}\")\n",
    "# print(f\"Total time steps: {len(times)}\")\n",
    "# \n",
    "# if len(times) > 1:\n",
    "#     time_diff = times[1] - times[0]\n",
    "#     print(f\"Time step frequency: {time_diff}\")\n",
    "#     \n",
    "#     # Check if times are regular\n",
    "#     if len(times) > 2:\n",
    "#         time_diffs = np.diff(times)\n",
    "#         if np.allclose(time_diffs, time_diffs[0]):\n",
    "#             print(\"Times are regularly spaced\")\n",
    "#         else:\n",
    "#             print(\"Times are irregularly spaced\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now You Try**: After merging data, try:\n",
    "- Merging data from 3 or more files\n",
    "- Checking that merged data has the expected dimensions\n",
    "- Comparing variables before and after merging to ensure nothing was lost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Data Validation\n",
    "\n",
    "After loading, validate your data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check for NaN values\n",
    "# Uncomment after loading your data:\n",
    "# nan_count = ds['density'].isnull().sum().values\n",
    "# total_count = ds['density'].size\n",
    "# if nan_count > 0:\n",
    "#     print(f\"Found {nan_count} NaN values in density ({100*nan_count/total_count:.1f}%)\")\n",
    "# else:\n",
    "#     print(\"No NaN values in density\")\n",
    "\n",
    "# Check for reasonable value ranges\n",
    "# density_vals = ds['density'].values\n",
    "# valid_vals = density_vals[~np.isnan(density_vals)]\n",
    "# if len(valid_vals) > 0:\n",
    "#     print(f\"Density range: {valid_vals.min():.1f} to {valid_vals.max():.1f} kg/m\u00b3\")\n",
    "#     if valid_vals.min() < 0 or valid_vals.max() > 1000:\n",
    "#         print(\"Density values outside typical range (0-1000 kg/m\u00b3)\")\n",
    "#     else:\n",
    "#         print(\"Density values in reasonable range\")\n",
    "\n",
    "# temp_vals = ds['temperature'].values\n",
    "# valid_vals = temp_vals[~np.isnan(temp_vals)]\n",
    "# if len(valid_vals) > 0:\n",
    "#     print(f\"Temperature range: {valid_vals.min():.1f} to {valid_vals.max():.1f} \u00b0C\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Merging Profile and Meteorological Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note on Missing Data**: For comprehensive coverage of missing data detection, handling strategies (interpolation, filling, dropping), and cleaning pipelines, see **07_data_quality_and_cleaning.ipynb**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Check results\n",
    "print(f\"\\nOriginal missing: {ds['density'].isnull().sum().values}\")\n",
    "print(f\"After mean fill: {density_mean_filled.isnull().sum().values}\")\n",
    "print(f\"After median fill: {density_median_filled.isnull().sum().values}\")\n",
    "print(f\"After location mean fill: {density_location_filled.isnull().sum().values}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy 4: Dropping Missing Data\n",
    "\n",
    "Sometimes it's better to remove data with missing values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Drop missing data with different strategies\n",
    "\n",
    "# Strategy 1: Drop any layer with missing density\n",
    "density_no_missing_layers = ds['density'].dropna(dim='layer')\n",
    "print(f\"Drop missing layers: {density_no_missing_layers.dims.get('layer', 'N/A')} layers remaining\")\n",
    "\n",
    "# Strategy 2: Drop entire profiles with ANY missing values (aggressive)\n",
    "profiles_clean = ds.dropna(dim='layer', how='any')\n",
    "print(f\"Drop profiles with any missing: {profiles_clean.dims.get('time', 'N/A')} profiles remaining\")\n",
    "\n",
    "# Strategy 3: Drop only if ALL values are missing (conservative)\n",
    "profiles_partial = ds.dropna(dim='layer', how='all')\n",
    "print(f\"Drop only fully-missing layers: {profiles_partial.dims.get('time', 'N/A')} profiles remaining\")\n",
    "\n",
    "# Strategy 4: Drop specific time steps with missing data\n",
    "time_clean = ds.dropna(dim='time', how='any')\n",
    "print(f\"Drop time steps with any missing: {time_clean.dims.get('time', 'N/A')} time steps remaining\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy 5: Multi-Step Cleaning Pipeline\n",
    "\n",
    "For real-world data, combine multiple strategies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive cleaning pipeline for density data\n",
    "def clean_density_data(ds, variable='density'):\n",
    "    \"\"\"\n",
    "    Multi-step cleaning pipeline for density data.\n",
    "    \n",
    "    Steps:\n",
    "    1. Interpolate missing values along layer dimension\n",
    "    2. Forward/backward fill any remaining gaps\n",
    "    3. Fill with location-specific mean if still missing\n",
    "    4. Drop profiles with >50% missing data\n",
    "    \"\"\"\n",
    "    density = ds[variable].copy()\n",
    "    \n",
    "    # Step 1: Interpolate along layer dimension (for depth profiles)\n",
    "    density = density.interpolate_na(dim='layer', method='linear')\n",
    "    \n",
    "    # Step 2: Forward then backward fill\n",
    "    density = density.fillna(method='ffill', dim='layer')\n",
    "    density = density.fillna(method='bfill', dim='layer')\n",
    "    \n",
    "    # Step 3: Fill remaining with location-specific mean\n",
    "    location_mean = density.mean(dim=['time', 'layer', 'slope', 'realization'])\n",
    "    density = density.fillna(location_mean)\n",
    "    \n",
    "    # Step 4: Drop profiles with >50% missing (if any remain)\n",
    "    missing_per_profile = density.isnull().sum(dim='layer') / density.sizes['layer']\n",
    "    valid_profiles = missing_per_profile < 0.5\n",
    "    \n",
    "    # Apply filter (keep only valid profiles)\n",
    "    # Note: This is simplified - in practice you'd need to filter the dataset\n",
    "    \n",
    "    return density\n",
    "\n",
    "# Apply cleaning pipeline\n",
    "density_cleaned = clean_density_data(ds, 'density')\n",
    "print(\"Cleaning pipeline applied:\")\n",
    "print(f\"  Original missing: {ds['density'].isnull().sum().values}\")\n",
    "print(f\"  After cleaning: {density_cleaned.isnull().sum().values}\")\n",
    "print(f\"  Missing values removed: {ds['density'].isnull().sum().values - density_cleaned.isnull().sum().values}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Right Strategy\n",
    "\n",
    "**Use interpolation when:**\n",
    "- Missing values are in time series data\n",
    "- You have enough neighboring data points\n",
    "- Values change smoothly over time/depth\n",
    "\n",
    "**Use forward/backward fill when:**\n",
    "- Missing values are at edges (surface or bottom layers)\n",
    "- Values are relatively constant\n",
    "- Quick fix needed\n",
    "\n",
    "**Use statistical imputation when:**\n",
    "- Missing values are scattered randomly\n",
    "- You have good overall statistics\n",
    "- Interpolation isn't appropriate\n",
    "\n",
    "**Use dropping when:**\n",
    "- Missing data is extensive (>50% of profile)\n",
    "- Missing data indicates data quality issues\n",
    "- You have enough remaining data for analysis\n",
    "\n",
    "**Use multi-step pipeline when:**\n",
    "- Data has complex missing patterns\n",
    "- You need robust cleaning\n",
    "- Production data processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now You Try**: After learning about missing data handling, try:\n",
    "- Creating a validation report for your dataset showing missing data patterns\n",
    "- Applying different cleaning strategies and comparing results\n",
    "- Creating a custom cleaning function for your specific data needs\n",
    "- Validating that cleaned data produces reasonable analysis results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note on Data Type Optimization**: For comprehensive coverage of data type optimization, memory management, and performance tuning for large datasets, see **09_performance_and_storage.ipynb**.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}