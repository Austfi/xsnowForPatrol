{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04: Gradient and Temporal Analysis\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Austfi/xsnowForPatrol/blob/main/notebooks/04_advanced_analysis.ipynb)\n",
    "\n",
    "This notebook covers advanced snowpack analysis techniques including gradient analysis and temporal patterns.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Density gradient analysis to identify weak layers\n",
    "- Temperature gradient analysis for understanding metamorphism\n",
    "- Temporal analysis with rolling windows and rates of change\n",
    "- Comparing multiple locations\n",
    "\n",
    "> **Note**: For advanced xarray techniques like broadcasting, alignment, groupby, and resampling, see **08_advanced_xarray_techniques.ipynb**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation (For Colab Users)\n",
    "\n",
    "If you're using Google Colab, run the cell below to install xsnow and dependencies. If you're running locally and have already installed xsnow, you can skip this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install -q numpy pandas xarray matplotlib seaborn dask netcdf4\n",
    "%pip install -q git+https://gitlab.com/avacollabra/postprocessing/xsnow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xsnow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load sample data\n",
    "print(\"Loading xsnow sample data...\")\n",
    "try:\n",
    "    ds = xsnow.single_profile_timeseries()\n",
    "    print(\"âœ… Data loaded successfully\")\n",
    "    print(f\"Dataset dimensions: {dict(ds.dims)}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Error loading sample data: {e}\")\n",
    "    print(\"Make sure xsnow is properly installed:\")\n",
    "    print(\"  pip install git+https://gitlab.com/avacollabra/postprocessing/xsnow\")\n",
    "    raise\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Density Gradient Analysis\n",
    "\n",
    "Density gradients can help identify weak layers in the snowpack. Large changes in density with depth may indicate potential weak layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XArray Broadcasting (Automatic)\n",
    "\n",
    "xarray automatically handles broadcasting when working with DataArrays:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Add a constant to all density values\n",
    "# xarray automatically broadcasts the scalar to match the array shape\n",
    "density_plus_10 = ds['density'] + 10  # Adds 10 to every density value\n",
    "print(f\"Original density range: {ds['density'].min().values:.1f} - {ds['density'].max().values:.1f} kg/mÂ³\")\n",
    "print(f\"After adding 10: {density_plus_10.min().values:.1f} - {density_plus_10.max().values:.1f} kg/mÂ³\")\n",
    "\n",
    "# Example 2: Normalize density by mean (broadcasting division)\n",
    "mean_density = ds['density'].mean()\n",
    "normalized_density = ds['density'] / mean_density\n",
    "print(f\"Normalized density mean: {normalized_density.mean().values:.3f}\")\n",
    "\n",
    "# Example 3: Operations between arrays of different shapes\n",
    "# Surface density (no layer dimension) vs full density profile\n",
    "surface_density = ds['density'].isel(layer=0)\n",
    "density_diff_from_surface = ds['density'] - surface_density\n",
    "print(f\"Density difference from surface shape: {density_diff_from_surface.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NumPy Broadcasting Rules\n",
    "\n",
    "NumPy has specific broadcasting rules that are important to understand:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy broadcasting rules:\n",
    "# 1. Arrays are aligned from the right\n",
    "# 2. Dimensions of size 1 are \"stretched\" to match\n",
    "# 3. Missing dimensions are treated as size 1\n",
    "\n",
    "# Example 1: 1D array with scalar\n",
    "arr_1d = np.array([100, 200, 300, 400])  # Shape: (4,)\n",
    "scalar = 50\n",
    "result = arr_1d + scalar  # Broadcasts scalar to (4,)\n",
    "print(f\"1D + scalar: {result}\")\n",
    "\n",
    "# Example 2: 2D array with 1D array\n",
    "arr_2d = np.array([[100, 200], [300, 400]])  # Shape: (2, 2)\n",
    "arr_1d = np.array([10, 20])  # Shape: (2,)\n",
    "result = arr_2d + arr_1d  # Broadcasts (2,) to (1, 2) then to (2, 2)\n",
    "print(f\"2D + 1D:\\n{result}\")\n",
    "\n",
    "# Example 3: 3D array with 1D array (like snowpack data)\n",
    "# Simulate: (time, layer) array with (layer,) array\n",
    "arr_3d = np.random.rand(10, 5)  # 10 time steps, 5 layers\n",
    "arr_1d = np.array([0.1, 0.2, 0.3, 0.4, 0.5])  # 5 layers\n",
    "result = arr_3d + arr_1d  # Broadcasts (5,) to (1, 5) then to (10, 5)\n",
    "print(f\"3D + 1D shape: {result.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance: NumPy vs XArray Broadcasting\n",
    "\n",
    "For large arrays, NumPy operations can be faster, but xarray provides more convenience:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Get a subset for timing comparison\n",
    "density_subset = ds['density'].isel(location=0, slope=0, realization=0)\n",
    "\n",
    "# Method 1: XArray broadcasting (convenient, preserves metadata)\n",
    "start = time.time()\n",
    "result_xarray = density_subset + 10\n",
    "time_xarray = time.time() - start\n",
    "print(f\"XArray broadcasting: {time_xarray*1000:.3f} ms\")\n",
    "\n",
    "# Method 2: NumPy broadcasting (potentially faster for large arrays)\n",
    "start = time.time()\n",
    "density_numpy = density_subset.values\n",
    "result_numpy = density_numpy + 10\n",
    "time_numpy = time.time() - start\n",
    "print(f\"NumPy broadcasting: {time_numpy*1000:.3f} ms\")\n",
    "\n",
    "# Note: For small arrays, the difference is negligible\n",
    "# For very large arrays, NumPy can be faster, but you lose coordinate information\n",
    "print(f\"\\nSpeedup: {time_xarray/time_numpy:.2f}x\")\n",
    "print(\"Note: XArray preserves coordinates and metadata, NumPy is just the raw array\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical Example: Efficient Computation with Broadcasting\n",
    "\n",
    "Use broadcasting to efficiently compute operations across dimensions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Normalize each profile by its mean density\n",
    "# This requires computing mean for each profile, then broadcasting\n",
    "\n",
    "# Get mean density for each profile (removes 'layer' dimension)\n",
    "mean_density_per_profile = ds['density'].mean(dim='layer')  # Shape: (location, time, slope, realization)\n",
    "\n",
    "# Broadcast to subtract from full density array\n",
    "# xarray automatically broadcasts (location, time, slope, realization) to (location, time, slope, realization, layer)\n",
    "normalized_density = ds['density'] - mean_density_per_profile\n",
    "\n",
    "print(f\"Original density shape: {ds['density'].shape}\")\n",
    "print(f\"Mean per profile shape: {mean_density_per_profile.shape}\")\n",
    "print(f\"Normalized density shape: {normalized_density.shape}\")\n",
    "print(f\"Normalized mean (should be ~0): {normalized_density.mean().values:.6f}\")\n",
    "\n",
    "# Example 2: Compute anomaly from time mean\n",
    "# Mean density over time for each location\n",
    "mean_density_over_time = ds['density'].mean(dim='time')  # Shape: (location, slope, realization, layer)\n",
    "\n",
    "# Anomaly = current - time mean\n",
    "density_anomaly = ds['density'] - mean_density_over_time\n",
    "print(f\"\\nDensity anomaly shape: {density_anomaly.shape}\")\n",
    "print(f\"Anomaly mean: {density_anomaly.mean().values:.3f} kg/mÂ³\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When to Use NumPy Broadcasting Directly\n",
    "\n",
    "Use NumPy broadcasting when:\n",
    "- **Performance is critical**: Very large arrays where speed matters\n",
    "- **No coordinate information needed**: You're doing pure numerical computation\n",
    "- **Integration with other libraries**: Libraries that expect NumPy arrays\n",
    "- **Custom algorithms**: Complex operations not easily expressed in xarray\n",
    "\n",
    "Use XArray broadcasting when:\n",
    "- **Preserve metadata**: You need coordinates, units, and attributes\n",
    "- **Labeled operations**: You want to use dimension names instead of indices\n",
    "- **Automatic alignment**: Working with multiple datasets that need alignment\n",
    "- **Readability**: Code is clearer with labeled dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now You Try**: After learning about broadcasting, try:\n",
    "- Using NumPy broadcasting to normalize density values manually\n",
    "- Comparing performance of xarray vs NumPy operations on a large subset\n",
    "- Creating a normalized temperature variable using broadcasting\n",
    "- Using broadcasting to compute layer-wise differences from surface values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment: Putting Data on the Same Grid\n",
    "\n",
    "**Alignment** ensures that when you perform operations between datasets, they're aligned by their coordinates (location, time, etc.). xarray automatically aligns data by matching coordinate values, which is essential when working with multiple data sources or different time periods.\n",
    "\n",
    "**What you'll see**: The examples below demonstrate how xarray aligns datasets with different coordinate values, ensuring operations are performed on matching data points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.5: Pandas GroupBy Operations\n",
    "\n",
    "The **groupby** operation is a powerful pattern for analyzing data by groups. It follows a \"split-apply-combine\" strategy:\n",
    "1. **Split**: Divide data into groups based on some criteria\n",
    "2. **Apply**: Perform operations on each group\n",
    "3. **Combine**: Merge results back together\n",
    "\n",
    "**What you'll see**: The examples below show how to group snowpack data by location, time periods, or other dimensions to compute statistics and compare groups.\n",
    "\n",
    "### Understanding GroupBy\n",
    "\n",
    "GroupBy allows you to:\n",
    "- Compare statistics across different locations\n",
    "- Analyze seasonal patterns (group by month)\n",
    "- Compare different slope aspects\n",
    "- Aggregate data in meaningful ways\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping by Location\n",
    "\n",
    "Compare statistics across different locations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by location and compute mean density\n",
    "mean_density_by_location = ds['density'].groupby('location').mean(dim=['time', 'layer'])\n",
    "print(\"Mean density by location:\")\n",
    "for loc in mean_density_by_location.location.values:\n",
    "    mean_dens = mean_density_by_location.sel(location=loc).values\n",
    "    print(f\"  Location {loc}: {mean_dens:.1f} kg/mÂ³\")\n",
    "\n",
    "# Group by location and compute multiple statistics\n",
    "density_stats = ds['density'].groupby('location').agg(['mean', 'std', 'min', 'max'], dim=['time', 'layer'])\n",
    "print(\"\\nDensity statistics by location:\")\n",
    "print(density_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping by Time Periods\n",
    "\n",
    "Analyze seasonal or monthly patterns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by month to see seasonal patterns\n",
    "# First, ensure time coordinate is datetime\n",
    "mean_density_by_month = ds['density'].groupby('time.month').mean(dim=['location', 'layer', 'time'])\n",
    "print(\"Mean density by month:\")\n",
    "for month in mean_density_by_month.month.values:\n",
    "    mean_dens = mean_density_by_month.sel(month=month).values\n",
    "    month_name = pd.Timestamp(2000, month, 1).strftime('%B')\n",
    "    print(f\"  {month_name}: {mean_dens:.1f} kg/mÂ³\")\n",
    "\n",
    "# Group by day of year (1-365) to see annual patterns\n",
    "mean_density_by_doy = ds['density'].groupby('time.dayofyear').mean(dim=['location', 'layer', 'time'])\n",
    "print(f\"\\nMean density by day of year: {len(mean_density_by_doy)} days\")\n",
    "print(f\"  Range: {mean_density_by_doy.dayofyear.min().values} - {mean_density_by_doy.dayofyear.max().values}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Grouping Keys\n",
    "\n",
    "Group by multiple dimensions simultaneously:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by both location and month\n",
    "# This gives you mean density for each location in each month\n",
    "density_by_location_month = ds['density'].groupby(['location', 'time.month']).mean(dim=['layer', 'time'])\n",
    "print(\"Mean density by location and month:\")\n",
    "print(f\"  Shape: {density_by_location_month.shape}\")\n",
    "print(f\"  Dimensions: {density_by_location_month.dims}\")\n",
    "\n",
    "# Access specific location and month\n",
    "if len(density_by_location_month.location) > 0 and len(density_by_location_month.month) > 0:\n",
    "    first_loc = density_by_location_month.location.values[0]\n",
    "    first_month = density_by_location_month.month.values[0]\n",
    "    value = density_by_location_month.sel(location=first_loc, month=first_month).values\n",
    "    print(f\"  Example: Location {first_loc}, Month {first_month}: {value:.1f} kg/mÂ³\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Aggregation Functions\n",
    "\n",
    "Apply custom functions to each group:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function to compute the range (max - min)\n",
    "def density_range(x):\n",
    "    \"\"\"Compute the range of density values.\"\"\"\n",
    "    return x.max(dim=['time', 'layer']) - x.min(dim=['time', 'layer'])\n",
    "\n",
    "# Apply custom function to each location\n",
    "density_range_by_location = ds['density'].groupby('location').apply(density_range)\n",
    "print(\"Density range (max - min) by location:\")\n",
    "for loc in density_range_by_location.location.values:\n",
    "    range_val = density_range_by_location.sel(location=loc).values\n",
    "    print(f\"  Location {loc}: {range_val:.1f} kg/mÂ³\")\n",
    "\n",
    "# Another example: count of layers above threshold per location\n",
    "def count_dense_layers(x, threshold=300):\n",
    "    \"\"\"Count layers with density above threshold.\"\"\"\n",
    "    return (x > threshold).sum(dim=['time', 'layer'])\n",
    "\n",
    "dense_layer_count = ds['density'].groupby('location').apply(count_dense_layers, threshold=300)\n",
    "print(\"\\nCount of dense layers (> 300 kg/mÂ³) by location:\")\n",
    "for loc in dense_layer_count.location.values:\n",
    "    count = dense_layer_count.sel(location=loc).values\n",
    "    print(f\"  Location {loc}: {count:.0f} layers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Example: Comparing Seasonal Patterns\n",
    "\n",
    "Let's use groupby to compare how density changes seasonally across locations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by location and month, compute mean density\n",
    "seasonal_density = ds['density'].groupby(['location', 'time.month']).mean(dim=['layer', 'time'])\n",
    "\n",
    "# Visualize seasonal patterns\n",
    "if len(seasonal_density.location) > 0 and len(seasonal_density.month) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    for loc in seasonal_density.location.values[:3]:  # Show first 3 locations\n",
    "        monthly_values = [seasonal_density.sel(location=loc, month=m).values \n",
    "                         for m in seasonal_density.month.values]\n",
    "        ax.plot(seasonal_density.month.values, monthly_values, \n",
    "               marker='o', label=f'Location {loc}', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Month', fontsize=12)\n",
    "    ax.set_ylabel('Mean Density (kg/mÂ³)', fontsize=12)\n",
    "    ax.set_title('Seasonal Density Patterns by Location', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… Seasonal patterns visualized!\")\n",
    "else:\n",
    "    print(\"Note: Insufficient data for seasonal comparison\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now You Try**: After learning about groupby operations, try:\n",
    "- Grouping temperature by month and finding the coldest month\n",
    "- Grouping by location and computing the standard deviation of density\n",
    "- Creating a custom function to find the month with the highest snow height for each location\n",
    "- Grouping by both location and slope to compare different aspects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Automatic alignment by coordinates\n",
    "# When you select different subsets, xarray aligns them automatically\n",
    "ds_location_0 = ds.isel(location=0)\n",
    "ds_location_1 = ds.isel(location=1)\n",
    "\n",
    "# Even though these have different location coordinates, operations align automatically\n",
    "# by matching time, slope, realization, and layer coordinates\n",
    "density_diff = ds_location_0['density'] - ds_location_1['density']\n",
    "print(f\"Density difference shape: {density_diff.dims}\")\n",
    "print(f\"Mean difference: {density_diff.mean().values:.2f} kg/mÂ³\")\n",
    "\n",
    "# Example 2: Alignment when time coordinates differ\n",
    "# Select different time ranges\n",
    "times = ds.coords['time'].values\n",
    "ds_first_half = ds.sel(time=slice(times[0], times[len(times)//2]))\n",
    "ds_second_half = ds.sel(time=slice(times[len(times)//2], times[-1]))\n",
    "\n",
    "# Operations between these will align by matching coordinates (location, slope, etc.)\n",
    "# but time won't match, so the result will have the union of time coordinates\n",
    "print(f\"\\nFirst half time range: {ds_first_half.coords['time'].values[0]} to {ds_first_half.coords['time'].values[-1]}\")\n",
    "print(f\"Second half time range: {ds_second_half.coords['time'].values[0]} to {ds_second_half.coords['time'].values[-1]}\")\n",
    "\n",
    "# Example 3: Manual alignment using xarray's align function\n",
    "import xarray as xr\n",
    "\n",
    "# Create two datasets with slightly different coordinates\n",
    "# (In practice, this might happen when loading data from different sources)\n",
    "ds1 = ds.isel(location=0, time=slice(0, 10))\n",
    "ds2 = ds.isel(location=0, time=slice(5, 15))  # Overlapping but different time range\n",
    "\n",
    "# Align them to have the same coordinates (intersection by default)\n",
    "ds1_aligned, ds2_aligned = xr.align(ds1, ds2, join='inner')  # 'inner' = intersection\n",
    "print(f\"\\nAfter alignment:\")\n",
    "print(f\"ds1_aligned time range: {ds1_aligned.coords['time'].values[0]} to {ds1_aligned.coords['time'].values[-1]}\")\n",
    "print(f\"ds2_aligned time range: {ds2_aligned.coords['time'].values[0]} to {ds2_aligned.coords['time'].values[-1]}\")\n",
    "\n",
    "# Now operations are straightforward\n",
    "density_comparison = ds1_aligned['density'] - ds2_aligned['density']\n",
    "print(f\"Comparison shape: {density_comparison.dims}\")\n",
    "\n",
    "print(\"\\nâœ… Key takeaway: xarray automatically handles alignment in most cases!\")\n",
    "print(\"   You only need manual alignment when working with datasets from different sources.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now You Try**: After learning about broadcasting and alignment, try:\n",
    "- Creating a normalized density variable by dividing by the mean density across all locations\n",
    "- Comparing temperature between two different time periods (hint: use `.sel()` with time slices)\n",
    "- Adding a constant value to all density measurements and seeing how it affects other calculations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Density Gradient Analysis\n",
    "\n",
    "Density gradients can help identify weak layers in the snowpack. Large changes in density with depth may indicate potential weak layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute density gradient (change in density with depth)\n",
    "# Large gradients can indicate weak layers\n",
    "density_gradient = ds['density'].diff(dim='layer')\n",
    "print(f\"Density gradient shape: {density_gradient.shape}\")\n",
    "\n",
    "# Select a single profile to visualize\n",
    "profile = ds.isel(location=0, time=-1, slope=0, realization=0)\n",
    "depth = -profile.coords['z'].values\n",
    "density = profile['density'].values\n",
    "gradient = density_gradient.isel(location=0, time=-1, slope=0, realization=0).values\n",
    "\n",
    "# Plot density and gradient\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "ax1.plot(density, depth, 'b-', linewidth=2)\n",
    "ax1.set_xlabel('Density (kg/mÂ³)')\n",
    "ax1.set_ylabel('Depth (m)')\n",
    "ax1.set_title('Density Profile')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "ax2.plot(gradient, depth[:-1], 'r-', linewidth=2)\n",
    "ax2.axvline(x=0, color='k', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Density Gradient (kg/mÂ³ per layer)')\n",
    "ax2.set_ylabel('Depth (m)')\n",
    "ax2.set_title('Density Gradient')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute density gradient (change in density with depth)\n",
    "# Large gradients can indicate weak layers\n",
    "density_gradient = ds['density'].diff(dim='layer')\n",
    "print(f\"Density gradient shape: {density_gradient.shape}\")\n",
    "\n",
    "# Select a single profile to visualize\n",
    "profile = ds.isel(location=0, time=-1, slope=0, realization=0)\n",
    "depth = -profile.coords['z'].values\n",
    "density = profile['density'].values\n",
    "gradient = density_gradient.isel(location=0, time=-1, slope=0, realization=0).values\n",
    "\n",
    "# Plot density and gradient\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "ax1.plot(density, depth, 'b-', linewidth=2)\n",
    "ax1.set_xlabel('Density (kg/mÂ³)')\n",
    "ax1.set_ylabel('Depth (m)')\n",
    "ax1.set_title('Density Profile')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "ax2.plot(gradient, depth[:-1], 'r-', linewidth=2)\n",
    "ax2.axvline(x=0, color='k', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Density Gradient (kg/mÂ³ per layer)')\n",
    "ax2.set_ylabel('Depth (m)')\n",
    "ax2.set_title('Density Gradient')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now You Try**: After running the gradient analysis above, try:\n",
    "- Computing density gradient for a different time step\n",
    "- Finding layers with density gradient > 50 kg/mÂ³ per layer\n",
    "- Comparing density gradients between different locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get snow height time series\n",
    "hs_series = ds['HS'].isel(location=0, slope=0, realization=0)\n",
    "\n",
    "# Resample to daily averages (1D = 1 day)\n",
    "# This averages all values within each day\n",
    "hs_daily = hs_series.resample(time='1D').mean()\n",
    "print(f\"Original time steps: {len(hs_series.time)}\")\n",
    "print(f\"After daily resampling: {len(hs_daily.time)}\")\n",
    "print(f\"Time frequency: {hs_daily.time.values[1] - hs_daily.time.values[0]}\")\n",
    "\n",
    "# Compare original vs resampled\n",
    "times_orig = ds.coords['time'].values\n",
    "times_daily = hs_daily.time.values\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(times_orig, hs_series.values, 'b-', alpha=0.3, label='Original', linewidth=1)\n",
    "ax.plot(times_daily, hs_daily.values, 'r-', marker='o', label='Daily average', linewidth=2, markersize=4)\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Snow Height (m)', fontsize=12)\n",
    "ax.set_title('Snow Height: Original vs Daily Resampled', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling to Weekly and Monthly Averages\n",
    "\n",
    "Resample to longer time periods:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample to weekly averages (7D = 7 days)\n",
    "hs_weekly = hs_series.resample(time='7D').mean()\n",
    "print(f\"Weekly resampling: {len(hs_weekly.time)} time steps\")\n",
    "\n",
    "# Resample to monthly averages (1M = 1 month)\n",
    "hs_monthly = hs_series.resample(time='1M').mean()\n",
    "print(f\"Monthly resampling: {len(hs_monthly.time)} time steps\")\n",
    "\n",
    "# Compare different resampling frequencies\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(times_orig, hs_series.values, 'b-', alpha=0.2, label='Original', linewidth=1)\n",
    "ax.plot(hs_daily.time.values, hs_daily.values, 'g-', marker='o', label='Daily', linewidth=1.5, markersize=3)\n",
    "ax.plot(hs_weekly.time.values, hs_weekly.values, 'orange', marker='s', label='Weekly', linewidth=2, markersize=5)\n",
    "ax.plot(hs_monthly.time.values, hs_monthly.values, 'r-', marker='^', label='Monthly', linewidth=2.5, markersize=6)\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Snow Height (m)', fontsize=12)\n",
    "ax.set_title('Snow Height: Different Resampling Frequencies', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Aggregation Methods\n",
    "\n",
    "You can use different aggregation methods when resampling:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample with different aggregation methods\n",
    "hs_daily_mean = hs_series.resample(time='1D').mean()    # Average\n",
    "hs_daily_max = hs_series.resample(time='1D').max()      # Maximum\n",
    "hs_daily_min = hs_series.resample(time='1D').min()      # Minimum\n",
    "hs_daily_sum = hs_series.resample(time='1D').sum()      # Sum (useful for cumulative variables)\n",
    "\n",
    "print(\"Daily resampling with different methods:\")\n",
    "print(f\"  Mean: {hs_daily_mean.values[0]:.3f} m\")\n",
    "print(f\"  Max:  {hs_daily_max.values[0]:.3f} m\")\n",
    "print(f\"  Min:  {hs_daily_min.values[0]:.3f} m\")\n",
    "print(f\"  Sum:  {hs_daily_sum.values[0]:.3f} m\")\n",
    "\n",
    "# Visualize mean, max, and min\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(hs_daily_mean.time.values, hs_daily_mean.values, 'b-', label='Daily mean', linewidth=2)\n",
    "ax.plot(hs_daily_max.time.values, hs_daily_max.values, 'r--', label='Daily max', linewidth=1.5, alpha=0.7)\n",
    "ax.plot(hs_daily_min.time.values, hs_daily_min.values, 'g--', label='Daily min', linewidth=1.5, alpha=0.7)\n",
    "ax.fill_between(hs_daily_mean.time.values, hs_daily_min.values, hs_daily_max.values, \n",
    "                alpha=0.2, color='gray', label='Daily range')\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Snow Height (m)', fontsize=12)\n",
    "ax.set_title('Daily Snow Height: Mean, Max, and Min', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling Layer-Level Data\n",
    "\n",
    "You can also resample layer-level variables (like density or temperature):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample surface layer temperature to daily averages\n",
    "surface_temp = ds['temperature'].isel(location=0, layer=0, slope=0, realization=0)\n",
    "temp_daily = surface_temp.resample(time='1D').mean()\n",
    "\n",
    "print(f\"Original temperature time steps: {len(surface_temp.time)}\")\n",
    "print(f\"Daily resampled: {len(temp_daily.time)}\")\n",
    "\n",
    "# Resample density for a specific layer\n",
    "# Get mean density across all layers, then resample\n",
    "mean_density = ds['density'].mean(dim='layer').isel(location=0, slope=0, realization=0)\n",
    "density_daily = mean_density.resample(time='1D').mean()\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "times_orig = surface_temp.time.values\n",
    "times_daily = temp_daily.time.values\n",
    "\n",
    "ax1.plot(times_orig, surface_temp.values, 'b-', alpha=0.3, label='Original', linewidth=1)\n",
    "ax1.plot(times_daily, temp_daily.values, 'r-', marker='o', label='Daily average', linewidth=2, markersize=4)\n",
    "ax1.set_ylabel('Temperature (Â°C)', fontsize=12)\n",
    "ax1.set_title('Surface Temperature', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(density_daily.time.values, density_daily.values, 'g-', marker='s', linewidth=2, markersize=4)\n",
    "ax2.set_xlabel('Date', fontsize=12)\n",
    "ax2.set_ylabel('Mean Density (kg/mÂ³)', fontsize=12)\n",
    "ax2.set_title('Daily Average Mean Density', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Irregular Time Series\n",
    "\n",
    "If your data has irregular time spacing, resample can still work:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample handles irregular time series automatically\n",
    "# It creates regular time bins and aggregates data within each bin\n",
    "\n",
    "# Example: Resample to daily, handling missing days\n",
    "hs_daily_filled = hs_series.resample(time='1D').mean()\n",
    "\n",
    "# Check for missing days (NaN values)\n",
    "missing_days = hs_daily_filled.isnull().sum().values\n",
    "print(f\"Missing days in resampled data: {missing_days}\")\n",
    "\n",
    "# Fill missing days using interpolation\n",
    "hs_daily_interpolated = hs_daily_filled.interpolate_na(dim='time', method='linear')\n",
    "print(f\"After interpolation: {hs_daily_interpolated.isnull().sum().values} missing values\")\n",
    "\n",
    "# Or forward fill (use previous value)\n",
    "hs_daily_ffill = hs_daily_filled.fillna(method='ffill')\n",
    "print(f\"After forward fill: {hs_daily_ffill.isnull().sum().values} missing values\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Resampling Frequencies\n",
    "\n",
    "Here are common time frequency strings for resampling:\n",
    "\n",
    "- `'1H'` - Hourly\n",
    "- `'1D'` - Daily\n",
    "- `'7D'` or `'1W'` - Weekly\n",
    "- `'1M'` - Monthly (calendar month)\n",
    "- `'1Q'` - Quarterly\n",
    "- `'1Y'` or `'1A'` - Yearly/Annual\n",
    "\n",
    "You can also use offsets like `'2D'` (every 2 days) or `'3H'` (every 3 hours).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now You Try**: After learning about resampling, try:\n",
    "- Resampling snow height to weekly averages and comparing with daily averages\n",
    "- Resampling temperature to monthly means and finding the warmest month\n",
    "- Using different aggregation methods (max, min, std) when resampling\n",
    "- Resampling multiple variables simultaneously\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature gradient = change in temperature per unit depth\n",
    "# Large gradients (> 10 K/m) can lead to faceting (weak layer formation)\n",
    "\n",
    "# Compute gradient using z coordinate\n",
    "temp_diff = ds['temperature'].diff(dim='layer')\n",
    "z_diff = ds.coords['z'].diff(dim='layer')\n",
    "temp_gradient = temp_diff / z_diff\n",
    "\n",
    "# Find layers with high gradients (> 10 K/m indicates faceting)\n",
    "high_gradient = temp_gradient > 10  # K/m\n",
    "n_high_gradient = high_gradient.sum().values\n",
    "print(f\"Found {n_high_gradient} layers with temperature gradient > 10 K/m\")\n",
    "\n",
    "# Visualize temperature gradient for a single profile\n",
    "profile = ds.isel(location=0, time=-1, slope=0, realization=0)\n",
    "depth = -profile.coords['z'].values\n",
    "tg_profile = temp_gradient.isel(location=0, time=-1, slope=0, realization=0)\n",
    "temp_profile = ds['temperature'].isel(location=0, time=-1, slope=0, realization=0)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "ax1.plot(temp_profile.values, depth, 'b-', linewidth=2)\n",
    "ax1.set_xlabel('Temperature (Â°C)')\n",
    "ax1.set_ylabel('Depth (m)')\n",
    "ax1.set_title('Temperature Profile')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "ax2.plot(tg_profile.values, depth[:-1], 'r-', linewidth=2)\n",
    "ax2.axvline(x=10, color='k', linestyle='--', alpha=0.5, label='10 K/m threshold')\n",
    "ax2.set_xlabel('Temperature Gradient (K/m)')\n",
    "ax2.set_ylabel('Depth (m)')\n",
    "ax2.set_title('Temperature Gradient')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Temperature Gradient\n",
    "\n",
    "First, let's compute the temperature gradient:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Temperature Gradient Analysis\n",
    "\n",
    "Temperature gradients are important for understanding metamorphism and weak layer formation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Proof by Example, Disproof by Counterexample\n",
    "\n",
    "**Proof by Example, Disproof by Counterexample Pattern**: Let's explore common misconceptions about temperature gradients and weak layers.\n",
    "\n",
    "**Example (Proof)**: High temperature gradients (> 10 K/m) often indicate faceting and weak layer formation. Let's see this in action:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: High temperature gradient leading to weak layer\n",
    "# Find profiles with high temperature gradients\n",
    "high_tg_profiles = temp_gradient > 10  # K/m\n",
    "\n",
    "# Count how many layers have high gradients\n",
    "n_high_tg = high_tg_profiles.sum().values\n",
    "print(f\"Found {n_high_tg} layers with temperature gradient > 10 K/m\")\n",
    "print(\"This supports the idea that high gradients can indicate weak layer formation.\")\n",
    "\n",
    "# Show an example profile with high gradient\n",
    "if n_high_tg > 0:\n",
    "    # Find first occurrence\n",
    "    high_tg_locations = high_tg_profiles.any(dim=['time', 'layer'])\n",
    "    print(f\"\\nExample: {high_tg_locations.sum().values} locations have high temperature gradients\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Counterexample (Disproof)**: However, high temperature gradients don't ALWAYS mean weak layers. Sometimes other factors matter more. Let's explore:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counterexample: High gradient but strong layer (due to other factors)\n",
    "# Example: High gradient with high density might still be strong\n",
    "high_tg_high_density = (temp_gradient > 10) & (ds['density'] > 300)\n",
    "n_strong_high_tg = high_tg_high_density.sum().values\n",
    "print(f\"Found {n_strong_high_tg} layers with high temp gradient BUT high density (> 300 kg/mÂ³)\")\n",
    "print(\"This shows that temperature gradient alone doesn't always indicate weak layers.\")\n",
    "print(\"Other factors (density, grain type, bonding) also matter!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Counterexample (Disproof)**: However, high temperature gradients don't ALWAYS mean weak layers. Sometimes other factors matter more. Let's explore:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counterexample: High gradient but strong layer (due to other factors)\n",
    "# Example: High gradient with high density might still be strong\n",
    "high_tg_high_density = (temp_gradient > 10) & (ds['density'] > 300)\n",
    "n_strong_high_tg = high_tg_high_density.sum().values\n",
    "print(f\"Found {n_strong_high_tg} layers with high temp gradient BUT high density (> 300 kg/mÂ³)\")\n",
    "print(\"This shows that temperature gradient alone doesn't always indicate weak layers.\")\n",
    "print(\"Other factors (density, grain type, bonding) also matter!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now You Try**: After running the temperature gradient analysis, try:\n",
    "- Finding profiles where high temperature gradient (> 10 K/m) occurs with low density (< 150 kg/mÂ³) - these are more likely weak layers\n",
    "- Comparing temperature gradients between different time periods\n",
    "- Plotting temperature gradient vs density to see the relationship\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample: Time-Based Resampling\n",
    "\n",
    "**Resample** allows you to resample time series data to different frequencies (daily, weekly, monthly, etc.). This is useful for smoothing data or computing statistics over time periods.\n",
    "\n",
    "**What you'll see**: The examples below show how to resample time series data to different frequencies and compute statistics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now You Try**: After learning about groupby and resample, try:\n",
    "- Grouping density by location and computing the standard deviation (hint: use `.std()`)\n",
    "- Resampling snow height to weekly averages (if your time frequency allows)\n",
    "- Grouping temperature by month and finding the coldest month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare mean density across locations\n",
    "mean_density_by_location = ds['density'].mean(dim=['time', 'layer'])\n",
    "print(\"Mean density by location:\")\n",
    "for i, loc in enumerate(ds.coords['location'].values):\n",
    "    mean_dens = mean_density_by_location.isel(location=i).values\n",
    "    print(f\"  Location {loc}: {mean_dens:.1f} kg/mÂ³\")\n",
    "\n",
    "# Compare snow height evolution\n",
    "print(\"\\nSnow height by location (last time step):\")\n",
    "for i, loc in enumerate(ds.coords['location'].values):\n",
    "    hs_loc = ds['HS'].isel(location=i, slope=0, realization=0, time=-1)\n",
    "    print(f\"  Location {loc}: {hs_loc.values:.2f} m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now You Try**: After comparing locations, try:\n",
    "- Comparing a different metric (e.g., temperature gradient or SWE) across locations\n",
    "- Finding which location has the most variable density over time\n",
    "- Creating a visualization comparing multiple locations side-by-side\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now You Try**: After comparing locations, try:\n",
    "- Comparing a different metric (e.g., temperature gradient or SWE) across locations\n",
    "- Finding which location has the most variable density over time\n",
    "- Creating a visualization comparing multiple locations side-by-side\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract snow height time series\n",
    "hs_series = ds['HS'].isel(location=0, slope=0, realization=0)\n",
    "times = ds.coords['time'].values\n",
    "\n",
    "# 7-day rolling average (smoothing)\n",
    "hs_7day = hs_series.rolling(time=7, center=True).mean()\n",
    "\n",
    "# Rate of change\n",
    "hs_rate = hs_series.diff(dim='time')\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "ax1.plot(times, hs_series.values, 'b-', alpha=0.5, label='Original')\n",
    "ax1.plot(times, hs_7day.values, 'r-', linewidth=2, label='7-day rolling average')\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('Snow Height (m)')\n",
    "ax1.set_title('Snow Height Time Series')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(times[1:], hs_rate.values[1:], 'g-', linewidth=2)\n",
    "ax2.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Time')\n",
    "ax2.set_ylabel('Rate of Change (m/time step)')\n",
    "ax2.set_title('Snow Height Rate of Change')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean rate of change: {hs_rate.mean().values:.4f} m/time step\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now You Try**: After running the temporal analysis, try:\n",
    "- Computing a different rolling window (try 3-day, 14-day, or 30-day averages)\n",
    "- Calculating the rate of change for a different variable (e.g., density or temperature)\n",
    "- Finding the time period with the fastest snow height increase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Comparing Multiple Locations\n",
    "\n",
    "When you have data from multiple locations, you can compare their characteristics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Example 1: Aligning datasets with different time ranges\n",
    "# Split data into two time periods\n",
    "times = ds.coords['time'].values\n",
    "mid_point = len(times) // 2\n",
    "\n",
    "ds_period1 = ds.sel(time=slice(times[0], times[mid_point]))\n",
    "ds_period2 = ds.sel(time=slice(times[mid_point], times[-1]))\n",
    "\n",
    "print(\"Original dataset time range:\")\n",
    "print(f\"  Start: {times[0]}\")\n",
    "print(f\"  End: {times[-1]}\")\n",
    "print(f\"\\nPeriod 1 time range: {ds_period1.coords['time'].values[0]} to {ds_period1.coords['time'].values[-1]}\")\n",
    "print(f\"Period 2 time range: {ds_period2.coords['time'].values[0]} to {ds_period2.coords['time'].values[-1]}\")\n",
    "\n",
    "# Align them (find intersection of coordinates)\n",
    "ds1_aligned, ds2_aligned = xr.align(ds_period1, ds_period2, join='inner')\n",
    "print(f\"\\nAfter alignment (inner join - intersection):\")\n",
    "print(f\"  Both datasets now have {len(ds1_aligned.coords['time'])} time steps\")\n",
    "\n",
    "# Example 2: Merging datasets\n",
    "# Merge two datasets (combines variables)\n",
    "# This is useful when you have different variables in different files\n",
    "ds_merged = xr.merge([ds_period1, ds_period2], join='outer')  # 'outer' = union\n",
    "print(f\"\\nMerged dataset time range: {ds_merged.coords['time'].values[0]} to {ds_merged.coords['time'].values[-1]}\")\n",
    "print(f\"  Total time steps: {len(ds_merged.coords['time'])}\")\n",
    "\n",
    "# Example 3: Concatenating along a dimension\n",
    "# Combine datasets along time dimension\n",
    "ds_concatenated = xr.concat([ds_period1, ds_period2], dim='time')\n",
    "print(f\"\\nConcatenated dataset time steps: {len(ds_concatenated.coords['time'])}\")\n",
    "print(f\"  Should equal sum of both periods: {len(ds_period1.coords['time']) + len(ds_period2.coords['time'])}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key functions:\")\n",
    "print(\"   - xr.align(): Align datasets to have matching coordinates\")\n",
    "print(\"   - xr.merge(): Combine datasets with different variables\")\n",
    "print(\"   - xr.concat(): Combine datasets along a dimension\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now You Try**: After learning about alignment operations, try:\n",
    "- Splitting your dataset into two time periods and aligning them\n",
    "- Merging two different selections of your dataset\n",
    "- Concatenating data from different locations along the location dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare mean density across locations\n",
    "mean_density_by_location = ds['density'].mean(dim=['time', 'layer'])\n",
    "print(\"Mean density by location:\")\n",
    "for i, loc in enumerate(ds.coords['location'].values):\n",
    "    mean_dens = mean_density_by_location.isel(location=i).values\n",
    "    print(f\"  Location {loc}: {mean_dens:.1f} kg/mÂ³\")\n",
    "\n",
    "# Compare snow height evolution\n",
    "print(\"\\nSnow height by location (last time step):\")\n",
    "for i, loc in enumerate(ds.coords['location'].values):\n",
    "    hs_loc = ds['HS'].isel(location=i, slope=0, realization=0, time=-1)\n",
    "    print(f\"  Location {loc}: {hs_loc.values:.2f} m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now You Try**: After running the temporal analysis, try:\n",
    "- Computing a different rolling window (try 3-day, 14-day, or 30-day averages)\n",
    "- Calculating the rate of change for a different variable (e.g., density or temperature)\n",
    "- Finding the time period with the fastest snow height increase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Advanced Temporal Analysis\n",
    "\n",
    "Time series analysis helps understand how the snowpack evolves over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract snow height time series\n",
    "hs_series = ds['HS'].isel(location=0, slope=0, realization=0)\n",
    "times = ds.coords['time'].values\n",
    "\n",
    "# 7-day rolling average (smoothing)\n",
    "hs_7day = hs_series.rolling(time=7, center=True).mean()\n",
    "\n",
    "# Rate of change\n",
    "hs_rate = hs_series.diff(dim='time')\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "ax1.plot(times, hs_series.values, 'b-', alpha=0.5, label='Original')\n",
    "ax1.plot(times, hs_7day.values, 'r-', linewidth=2, label='7-day rolling average')\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('Snow Height (m)')\n",
    "ax1.set_title('Snow Height Time Series')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(times[1:], hs_rate.values[1:], 'g-', linewidth=2)\n",
    "ax2.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Time')\n",
    "ax2.set_ylabel('Rate of Change (m/time step)')\n",
    "ax2.set_title('Snow Height Rate of Change')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean rate of change: {hs_rate.mean().values:.4f} m/time step\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Density gradients**: Identifying weak layers through density changes with depth\n",
    "2. **Temperature gradients**: Understanding faceting conditions (gradients > 10 K/m indicate weak layer formation)\n",
    "3. **Multi-location comparison**: Comparing statistics across different locations\n",
    "4. **Temporal analysis**: Using rolling averages to smooth time series and calculating rates of change\n",
    "\n",
    "## Key Techniques\n",
    "\n",
    "- **Gradients**: `ds['variable'].diff(dim='layer')` for layer-to-layer differences\n",
    "- **Temperature gradient**: `temp_diff / z_diff` to get gradient per unit depth\n",
    "- **Rolling windows**: `ds.rolling(time=7).mean()` for smoothing time series\n",
    "- **Rate of change**: `ds.diff(dim='time')` to see how values change over time\n",
    "- **Multi-location stats**: `.mean(dim=['time', 'layer'])` to aggregate across dimensions\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **05_working_with_custom_data.ipynb**: Load and analyze your own files\n",
    "- **06_extending_xsnow.ipynb**: Create custom analysis functions\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Compute temperature gradient and identify layers with gradients > 10 K/m\n",
    "2. Calculate a 7-day rolling average of snow height for a different location\n",
    "3. Compare mean density profiles across all locations in your dataset\n",
    "4. Plot the rate of change of temperature at a specific depth over time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
